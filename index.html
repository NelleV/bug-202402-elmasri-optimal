<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Maxime El Masri">
<meta name="author" content="Jérôme Morio">
<meta name="author" content="Florian Simatos">
<meta name="dcterms.date" content="2024-11-03">
<meta name="keywords" content="Rare event simulation, Parameter estimation, Importance sampling, Dimension reduction, Kullback–Leibler divergence, Projection">
<meta name="description" content="This document provides a dimension-reduction strategy in order to improve the performance of importance sampling in high dimensions.">

<title>BUG Optimal projection for parametric importance sampling in high dimensions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="published-elmasri-optimal_files/libs/clipboard/clipboard.min.js"></script>
<script src="published-elmasri-optimal_files/libs/quarto-html/quarto.js"></script>
<script src="published-elmasri-optimal_files/libs/quarto-html/popper.min.js"></script>
<script src="published-elmasri-optimal_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="published-elmasri-optimal_files/libs/quarto-html/anchor.min.js"></script>
<link href="published-elmasri-optimal_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="published-elmasri-optimal_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="published-elmasri-optimal_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="published-elmasri-optimal_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="published-elmasri-optimal_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="published-elmasri-optimal_files/libs/quarto-contrib/pseudocode-2.4/pseudocode.min.js"></script>
<link href="published-elmasri-optimal_files/libs/quarto-contrib/pseudocode-2.4/pseudocode.min.css" rel="stylesheet">
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: #FFFFFF;
      }

      .quarto-title-block .quarto-title-banner {
        color: #FFFFFF;
background: #034E79;
      }
</style>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title"><a href="https://computo.sfds.asso.fr">
        <img src="https://computo.sfds.asso.fr/assets/img/logo_notext_white.png" height="60px">
      </a> &nbsp; BUG Optimal projection for parametric importance sampling in high dimensions</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> source</button></div></div>
            <p><a href="http://creativecommons.org/licenses/by/4.0/"><img src="https://i.creativecommons.org/l/by/4.0/80x15.png" alt="Creative Commons BY License"></a>
ISSN 2824-7795</p>
            <div>
        <div class="description">
          <p>This document provides a dimension-reduction strategy in order to improve the performance of importance sampling in high dimensions.</p>
        </div>
      </div>
                </div>
  </div>
    
    <div class="quarto-title-meta-author">
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-heading">Affiliations</div>
          
          <div class="quarto-title-meta-contents">
        Maxime El Masri <a href="https://orcid.org/0000-0002-9127-4503" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.onera.fr/">ONERA/DTIS</a>, <a href="https://www.isae-supaero.fr/">ISAE-SUPAERO</a>, <a href="https://www.univ-toulouse.fr/">Université de Toulouse</a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://www.onera.fr/en/staff/jerome-morio?destination=node/981">Jérôme Morio</a> <a href="https://orcid.org/0000-0002-8811-8956" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.onera.fr/">ONERA/DTIS</a>, <a href="https://www.univ-toulouse.fr/">Université de Toulouse</a>
                </p>
            </div>
            <div class="quarto-title-meta-contents">
        <a href="https://pagespro.isae-supaero.fr/florian-simatos/">Florian Simatos</a> 
      </div>
          
          <div class="quarto-title-meta-contents">
              <p class="affiliation">
                  <a href="https://www.isae-supaero.fr/">ISAE-SUPAERO</a>, <a href="https://www.univ-toulouse.fr/">Université de Toulouse</a>
                </p>
            </div>
        </div>
                    
  <div class="quarto-title-meta">
                                
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 3, 2024</p>
      </div>
    </div>
                                    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">April 7, 2024</p>
      </div>
    </div>
      
                  
      <div>
      <div class="quarto-title-meta-heading">Keywords</div>
      <div class="quarto-title-meta-contents">
        <p class="date">Rare event simulation, Parameter estimation, Importance sampling, Dimension reduction, Kullback–Leibler divergence, Projection</p>
      </div>
    </div>
    
    <div>
      <div class="quarto-title-meta-heading">Status</div>
      <div class="quarto-title-meta-contents">
              <a href="https://github.com/computo/optimal-projection-IS"><img src="https://github.com/computo/optimal-projection-IS/actions/workflows/build.yml/badge.svg" alt="build status"></a>
                  </div>
    </div>

  </div>
                                                
  <div>
    <div class="abstract">
    <div class="abstract-title">Abstract</div>
      <p>We propose a dimension reduction strategy in order to improve the performance of importance sampling in high dimensions. The idea is to estimate variance terms in a small number of suitably chosen directions. We first prove that the optimal directions, i.e., the ones that minimize the Kullback–Leibler divergence with the optimal auxiliary density, are the eigenvectors associated with extreme (small or large) eigenvalues of the optimal covariance matrix. We then perform extensive numerical experiments showing that as dimension increases, these directions give estimations which are very close to optimal. Moreover, we demonstrate that the estimation remains accurate even when a simple empirical estimator of the covariance matrix is used to compute these directions. The theoretical and numerical results open the way for different generalizations, in particular the incorporation of such ideas in adaptive importance sampling schemes.</p>
    </div>
  </div>

  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#sec-IS" id="toc-sec-IS" class="nav-link" data-scroll-target="#sec-IS"><span class="header-section-number">2</span> Importance Sampling</a></li>
  <li><a href="#sec-main-result" id="toc-sec-main-result" class="nav-link" data-scroll-target="#sec-main-result"><span class="header-section-number">3</span> Efficient dimension reduction</a>
  <ul class="collapse">
  <li><a href="#sec-proj" id="toc-sec-proj" class="nav-link" data-scroll-target="#sec-proj"><span class="header-section-number">3.1</span> Projecting onto a low-dimensional subspace</a></li>
  <li><a href="#definition-of-the-function-ell" id="toc-definition-of-the-function-ell" class="nav-link" data-scroll-target="#definition-of-the-function-ell"><span class="header-section-number">3.2</span> Definition of the function <span class="math inline">\ell</span></a></li>
  <li><a href="#sec-main-result-positioning" id="toc-sec-main-result-positioning" class="nav-link" data-scroll-target="#sec-main-result-positioning"><span class="header-section-number">3.3</span> Main result of the paper</a></li>
  <li><a href="#sec-choicek" id="toc-sec-choicek" class="nav-link" data-scroll-target="#sec-choicek"><span class="header-section-number">3.4</span> Choice of the number of dimensions <span class="math inline">k</span></a></li>
  <li><a href="#sec-mm" id="toc-sec-mm" class="nav-link" data-scroll-target="#sec-mm"><span class="header-section-number">3.5</span> Theoretical result concerning the projection on <span class="math inline">\mathbf{m}^*</span></a></li>
  </ul></li>
  <li><a href="#sec-num-results-framework" id="toc-sec-num-results-framework" class="nav-link" data-scroll-target="#sec-num-results-framework"><span class="header-section-number">4</span> Computational framework</a>
  <ul class="collapse">
  <li><a href="#sec-def_proc" id="toc-sec-def_proc" class="nav-link" data-scroll-target="#sec-def_proc"><span class="header-section-number">4.1</span> Numerical procedure for IS estimate comparison</a></li>
  <li><a href="#sec-def_cov" id="toc-sec-def_cov" class="nav-link" data-scroll-target="#sec-def_cov"><span class="header-section-number">4.2</span> Choice of the auxiliary density <span class="math inline">g'</span> for the Gaussian model</a></li>
  </ul></li>
  <li><a href="#sec-test-cases" id="toc-sec-test-cases" class="nav-link" data-scroll-target="#sec-test-cases"><span class="header-section-number">5</span> Numerical results on five test cases</a>
  <ul class="collapse">
  <li><a href="#sec-sub:sum" id="toc-sec-sub:sum" class="nav-link" data-scroll-target="#sec-sub\:sum"><span class="header-section-number">5.1</span> Test case 1: one-dimensional optimal projection</a>
  <ul class="collapse">
  <li><a href="#evolution-of-the-partial-kl-divergence-and-spectrum" id="toc-evolution-of-the-partial-kl-divergence-and-spectrum" class="nav-link" data-scroll-target="#evolution-of-the-partial-kl-divergence-and-spectrum"><span class="header-section-number">5.1.1</span> Evolution of the partial KL divergence and spectrum</a></li>
  <li><a href="#numerical-results" id="toc-numerical-results" class="nav-link" data-scroll-target="#numerical-results"><span class="header-section-number">5.1.2</span> Numerical results</a></li>
  </ul></li>
  <li><a href="#sec-sub:parabol" id="toc-sec-sub:parabol" class="nav-link" data-scroll-target="#sec-sub\:parabol"><span class="header-section-number">5.2</span> Test case 2: projection in 2 directions</a>
  <ul class="collapse">
  <li><a href="#evolution-of-the-partial-kl-divergence-and-spectrum-1" id="toc-evolution-of-the-partial-kl-divergence-and-spectrum-1" class="nav-link" data-scroll-target="#evolution-of-the-partial-kl-divergence-and-spectrum-1"><span class="header-section-number">5.2.1</span> Evolution of the partial KL divergence and spectrum</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="published-elmasri-optimal.pdf"><i class="bi bi-file-pdf"></i>PDF (computo)</a></li></ul></div></nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction</h1>
<p>Importance Sampling (IS) is a stochastic method to estimate integrals of the form <span class="math inline">\mathcal{E} = \int \phi(\mathbf{x})f(\mathbf{x})\textrm{d} \mathbf{x}</span> with a black-box function <span class="math inline">\phi</span> and a probability density function (pdf) <span class="math inline">f</span>. It rests upon the choice of an auxiliary density which can significantly improve the estimation compared to the naive Monte Carlo (MC) method <span class="citation" data-cites="AgapiouEtAl_ImportanceSamplingIntrinsic_2017">(<a href="#ref-AgapiouEtAl_ImportanceSamplingIntrinsic_2017" role="doc-biblioref">Agapiou et al. 2017</a>)</span>, <span class="citation" data-cites="OwenZhou_SafeEffectiveImportance_2000">(<a href="#ref-OwenZhou_SafeEffectiveImportance_2000" role="doc-biblioref">Owen and Zhou 2000</a>)</span>. The theoretical optimal IS density, also called zero-variance density, is defined by <span class="math inline">\phi f / \mathcal{E}</span> when <span class="math inline">\phi</span> is a positive function. This density is not available in practice as it involves the unknown integral <span class="math inline">\mathcal{E}</span>, but a classical strategy consists in searching for an optimal approximation in a parametric family of densities. By minimising a “distance” to the optimal IS density, such as the Kullback–Leibler divergence, one can find optimal parameters in this family to get an efficient sampling pdf. Adaptive Importance Sampling (AIS) algorithms, such as the Mixture Population Monte Carlo method <span class="citation" data-cites="CappeEtAl_AdaptiveImportanceSampling_2008">(<a href="#ref-CappeEtAl_AdaptiveImportanceSampling_2008" role="doc-biblioref">Cappé et al. 2008</a>)</span>, the Adaptive Multiple Importance Sampling method <span class="citation" data-cites="CornuetEtAl_AdaptiveMultipleImportance_2012">(<a href="#ref-CornuetEtAl_AdaptiveMultipleImportance_2012" role="doc-biblioref">Cornuet et al. 2012</a>)</span>, or the Cross Entropy method <span class="citation" data-cites="RubinsteinKroese_CrossentropyMethodUnified_2011">(<a href="#ref-RubinsteinKroese_CrossentropyMethodUnified_2011" role="doc-biblioref">Rubinstein and Kroese 2011a</a>)</span>, estimate the optimal parameters adaptively by updating at intermediate levels <span class="citation" data-cites="BugalloEtAl_AdaptiveImportanceSampling_2017">(<a href="#ref-BugalloEtAl_AdaptiveImportanceSampling_2017" role="doc-biblioref">Bugallo et al. 2017</a>)</span>.</p>
<p>These techniques work very well, but only for moderate dimensions. In high dimensions, most of these techniques fail to give suitable parameters for two reasons:</p>
<ol type="1">
<li><p>the weight degeneracy problem, for which the self-normalized likelihood ratios (weights) in the IS densities degenerate in the sense that the largest one takes all the mass, while all other weights are negligible so that the final estimation essentially uses only one sample. See for instance <span class="citation" data-cites="BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008">(<a href="#ref-BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008" role="doc-biblioref">Bengtsson, Bickel, and Li 2008</a>)</span> for a theoretical analysis in the related context of particle filtering. The conditions under which importance sampling is applicable in high dimensions are notably investigated in a reliability context in <span class="citation" data-cites="AuBeck_ImportantSamplingHigh_2003">(<a href="#ref-AuBeck_ImportantSamplingHigh_2003" role="doc-biblioref">Au and Beck 2003</a>)</span>: it is remarked that the optimal covariance matrix should not deviate significantly from the identity matrix. <span class="citation" data-cites="El-LahamEtAl_RecursiveShrinkageCovariance_">(<a href="#ref-El-LahamEtAl_RecursiveShrinkageCovariance_" role="doc-biblioref">El-Laham, Elvira, and Bugallo 2019</a>)</span> tackle the weight degeneracy problem by applying a recursive shrinkage of the covariance matrix, which is constructed iteratively with a weighted sum of the sample covariance estimator and a biased, but more stable, estimator;</p></li>
<li><p>the intricate estimation of distribution parameters in high dimensions and particularly covariance matrices, whose size increases quadratically in the dimension <span class="citation" data-cites="AshurbekovaEtAl_OptimalShrinkageRobust_">(<a href="#ref-AshurbekovaEtAl_OptimalShrinkageRobust_" role="doc-biblioref">Ashurbekova et al. 2020</a>)</span>,<span class="citation" data-cites="LedoitWolf_WellconditionedEstimatorLargedimensional_2004">(<a href="#ref-LedoitWolf_WellconditionedEstimatorLargedimensional_2004" role="doc-biblioref">Ledoit and Wolf 2004</a>)</span>. Empirical covariance matrix estimate has notably a slow convergence rate in high dimensions <span class="citation" data-cites="fan2008high">(<a href="#ref-fan2008high" role="doc-biblioref">Fan, Fan, and Lv 2008</a>)</span>. For that purpose, dimension reduction techniques can be applied. The idea was recently put forth to reduce the effective dimension by only estimating these parameters (in particular the covariance matrix) in suitable directions <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span>, <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>. In this paper we delve deeper into this idea.</p></li>
</ol>
<p>The main contribution of the present paper is to identify the optimal directions in the fundamental case when the parametric family is Gaussian, and perform numerical simulations in order to understand how they behave in practice. In particular, we propose directions which, in contrast to the recent paper <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>, do not require the objective function to be differentiable, and moreover optimizes the Kullback–Leibler distance with the optimal density instead of simply an upper bound on it, as in <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>. In <a href="#sec-proj" class="quarto-xref">Section&nbsp;3.1</a> we elaborate in more details on the differences between the two approaches.</p>
<p>The paper is organised as follows: in <a href="#sec-IS" class="quarto-xref">Section&nbsp;2</a> we recall the foundations of IS. In <a href="#sec-main-result" class="quarto-xref">Section&nbsp;3</a>, we state our main theoretical result and we compare it with the current state-of-the-art. The proof of our theoretical result are given in Appendix; <a href="#sec-num-results-framework" class="quarto-xref">Section&nbsp;4</a> introduces the numerical framework that we have adopted, and <a href="#sec-test-cases" class="quarto-xref">Section&nbsp;5</a> presents the numerical results obtained on five different test cases to assess the efficiency of the directions that we propose. We conclude in <strong>?@sec-Ccl</strong> with a summary and research perspectives.</p>
</section>
<section id="sec-IS" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Importance Sampling</h1>
<p>We consider the problem of estimating the following integral: <span class="math display">
    \mathcal{E}=\mathbb{E}_f(\phi(\mathbf{X}))=\int \phi(\mathbf{x})f(\mathbf{x})\textrm{d} \mathbf{x},
    </span> where <span class="math inline">\mathbf{X}</span> is a random vector in <span class="math inline">\mathbb{R}^n</span> with standard Gaussian pdf <span class="math inline">f</span>, and <span class="math inline">\phi: \mathbb{R}^n\rightarrow\mathbb{R}_+</span> is a real-valued, non-negative function. The function <span class="math inline">\phi</span> is considered as a black-box function which is potentially expensive to evaluate, and this means that the number of calls to <span class="math inline">\phi</span> should be limited.</p>
<p>IS is an approach used to reduce the variance of the classical Monte Carlo estimator of <span class="math inline">\mathcal{E}</span>. The idea of IS is to generate a random sample <span class="math inline">\mathbf{X}_1,\ldots,\mathbf{X}_N</span> from an auxiliary density <span class="math inline">g</span>, instead of <span class="math inline">f</span>, and to compute the following estimator: <span id="eq-hatE"><span class="math display">
    \widehat{\mathcal{E}_N}=\frac{1}{N}\sum_{i=1}^N \phi(\mathbf{X}_i)L(\mathbf{X}_i),
     \tag{1}</span></span> with <span class="math inline">L=f/g</span> the likelihood ratio, or importance weight, and the auxiliary density <span class="math inline">g</span>, also called importance sampling density, is such that <span class="math inline">g(\mathbf{x})=0</span> implies <span class="math inline">\phi(\mathbf{x}) f(\mathbf{x})=0</span> for every <span class="math inline">\mathbf{x}</span> (which makes the product <span class="math inline">\phi L</span> well-defined). This estimator is consistent and unbiased but its accuracy strongly depends on the choice of the auxiliary density <span class="math inline">g</span>. It is well known that the optimal choice for <span class="math inline">g</span> is <span class="citation" data-cites="bucklew2013introduction">(<a href="#ref-bucklew2013introduction" role="doc-biblioref">Bucklew 2013</a>)</span> <span class="math display">
    g^*(\mathbf{x})=\dfrac{\phi(\mathbf{x})f(\mathbf{x})}{\mathcal{E}}, \ \mathbf{x}\in\mathbb{R}^n.
    </span> Indeed, for this choice we have <span class="math inline">\phi L = \mathcal{E}</span> and so <span class="math inline">\widehat{\mathcal{E}}_N</span> is actually the deterministic estimator <span class="math inline">\mathcal{E}</span>. For this reason, <span class="math inline">g^*</span> is sometimes called zero-variance density, a terminology that we will adopt here. Of course, <span class="math inline">g^*</span> is only of theoretical interest as it depends on the unknown integral <span class="math inline">\mathcal{E}</span>. However, it gives an idea of good choices for the auxiliary density <span class="math inline">g</span>, and we will seek to approximate <span class="math inline">g^*</span> by an auxiliary density that minimizes a distance between <span class="math inline">g^*</span> and a given parametric family of densities.</p>
<p>In this paper, the parametric family of densities is the Gaussian family <span class="math inline">\{g_{\mathbf{m}, \mathbf{\Sigma}}: \mathbf{m} \in \mathbb{R}^n, \mathbf{\Sigma} \in \mathcal{S}^+_n\}</span>, where <span class="math inline">g_{\mathbf{m}, \mathbf{\Sigma}}</span> denotes the Gaussian density with mean <span class="math inline">\mathbf{m} \in \mathbb{R}^n</span> and covariance matrix <span class="math inline">\mathbf{\Sigma} \in \mathcal{S}^+_n</span> with <span class="math inline">\mathcal{S}^+_n \subset \mathbb{R}^{n \times n}</span> the set of symmetric, positive-definite matrices: <span class="math display">
    g_{\mathbf{m},\mathbf{\Sigma}}(\mathbf{x})=\dfrac{1}{ (2\pi)^{n/2} \lvert \mathbf{\Sigma} \rvert^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\mathbf{m})^\top\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{m})\right), \ \mathbf{x} \in \mathbb{R}^n.
    </span> with <span class="math inline">\lvert \mathbf{\Sigma} \rvert</span> the determinant of <span class="math inline">\mathbf{\Sigma}</span>. Moreover, we will consider the Kullback–Leibler (KL) divergence to measure a “distance” between <span class="math inline">g^*</span> and <span class="math inline">g_{\mathbf{m}, \mathbf{\Sigma}}</span>. Recall that for two densities <span class="math inline">f</span> and <span class="math inline">h</span>, with <span class="math inline">f</span> absolutely continuous with respect to <span class="math inline">h</span>, the KL divergence <span class="math inline">D(f,h)</span> between <span class="math inline">f</span> and <span class="math inline">h</span> is defined by: <span class="math display">
    D(f,h)=\mathbb{E}_{f}\left[\log \left( \frac{f(\mathbf{X})}{h(\mathbf{X})} \right) \right] = \int \log \left( \frac{f(\mathbf{x})}{h(\mathbf{x})} \right)f(\mathbf{x}) \textrm{d} \mathbf{x}.
    </span> Thus, our goal is to approximate <span class="math inline">g^*</span> by <span class="math inline">g_{\mathbf{m}^*, \mathbf{\Sigma}^*}</span> with the optimal mean vector <span class="math inline">\mathbf{m}^*</span> and the optimal covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span> given by: <span id="eq-argminDkl"><span class="math display">
    (\mathbf{m}^*,\mathbf{\Sigma}^*) = \arg\min \left\{ D(g^*,g_{\mathbf{m},\mathbf{\Sigma}}): \mathbf{m} \in \mathbb{R}^n, \mathbf{\Sigma} \in \mathcal{S}_n^+ \right\}.
     \tag{2}</span></span> This optimization is in general convex and differentiable with respect to <span class="math inline">\mathbf{m}</span> and <span class="math inline">\mathbf{\Sigma}</span>. Moreover, the solution of <a href="#eq-argminDkl" class="quarto-xref">Equation&nbsp;2</a> can be computed analytically by cancelling the gradient. In the Gaussian case, it is thus proved that <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span> are simply the mean and variance of the zero-variance density <span class="citation" data-cites="RubinsteinKroese_CrossentropyMethodUnified_2011v2">(<a href="#ref-RubinsteinKroese_CrossentropyMethodUnified_2011v2" role="doc-biblioref">Rubinstein and Kroese 2011b</a>)</span>, <span class="citation" data-cites="RubinsteinKroese_SimulationMonteCarlo_2017v2">(<a href="#ref-RubinsteinKroese_SimulationMonteCarlo_2017v2" role="doc-biblioref">Rubinstein and Kroese 2017</a>)</span>: <span id="eq-mstar"><span class="math display">
    \mathbf{m}^*=\mathbb{E}_{g^*}(\mathbf{X}) \hspace{0.5cm} \text{ and } \hspace{0.5cm} \mathbf{\Sigma}^* = \textrm{Var}_{g^*} \left(\mathbf{X}\right).
     \tag{3}</span></span></p>
</section>
<section id="sec-main-result" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Efficient dimension reduction</h1>
<section id="sec-proj" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-proj"><span class="header-section-number">3.1</span> Projecting onto a low-dimensional subspace</h2>
<p>As <span class="math inline">g^*</span> is unknown, the optimal parameters <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span> given by <a href="#eq-mstar" class="quarto-xref">Equation&nbsp;3</a> are not directly computable. However, we can sample from the optimal density as it is known up to a multiplicative constant. Therefore, usual estimation schemes start with estimating <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span>, say through <span class="math inline">\widehat{\mathbf{m}}^*</span> and <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>, respectively, and then use these approximations to estimate <span class="math inline">\mathcal{E}</span> through <a href="#eq-hatE" class="quarto-xref">Equation&nbsp;1</a> with the auxiliary density <span class="math inline">g_{\widehat{\mathbf{m}}^*, \widehat{\mathbf{\Sigma}}^*}</span>. Although the estimation of <span class="math inline">\mathcal{E}</span> with the auxiliary density <span class="math inline">g_{\mathbf{m}^*, \mathbf{\Sigma}^*}</span> usually provides very good results, it is well-known that in high dimensions, the additional error induced by the estimations of <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span> severely degrades the accuracy of the final estimation <span class="citation" data-cites="PapaioannouEtAl_ImprovedCrossEntropybased_2019">(<a href="#ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" role="doc-biblioref">Papaioannou, Geyer, and Straub 2019</a>)</span>, <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>. The main problem lies in the estimation of <span class="math inline">\mathbf{\Sigma}^*</span> which, in dimension <span class="math inline">n</span>, involves the estimation of a quadratic (in the dimension) number of terms, namely <span class="math inline">n(n+1)/2</span>. Recently, the idea to overcome this problem by only evaluating variance terms in a small number of influential directions was explored in <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span> and <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>. In these two papers, the auxiliary covariance matrix <span class="math inline">\mathbf{\Sigma}</span> is modeled in the form <span id="eq-Sigmak"><span class="math display">
    \mathbf{\Sigma} = \sum_{i=1}^k (v_i-1) \mathbf{d}_i \mathbf{d}_i^\top + I_n
     \tag{4}</span></span> where the <span class="math inline">\mathbf{d}_i</span>’s are the <span class="math inline">k</span> orthonormal directions which are deemed influential. It is easy to check that <span class="math inline">\mathbf{\Sigma}</span> is the covariance matrix of the Gaussian vector <span class="math display"> v^{1/2}_1 Y_1 \mathbf{d}_1 + \cdots + v^{1/2}_k Y_k \mathbf{d}_k + Y_{k+1} \mathbf{d}_{k+1} + \cdots + Y_n \mathbf{d}_n </span> where the <span class="math inline">Y_i</span>’s are i.i.d. standard normal random variables (one-dimensional), and the <span class="math inline">n-k</span> vectors <span class="math inline">(\mathbf{d}_{k+1}, \ldots, \mathbf{d}_n)</span> complete <span class="math inline">(\mathbf{d}_1, \ldots, \mathbf{d}_k)</span> into an orthonormal basis. In particular, <span class="math inline">v_i</span> is the variance in the direction of <span class="math inline">\mathbf{d}_i</span>, i.e., <span class="math inline">v_i = \mathbf{d}_i^\top \mathbf{\Sigma} \mathbf{d}_i</span>. In <a href="#eq-Sigmak" class="quarto-xref">Equation&nbsp;4</a>, <span class="math inline">k</span> can be considered as the effective dimension in which variance terms are estimated. In other words, in <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span> and <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span>, the optimal variance parameter is not sought in <span class="math inline">\mathcal{S}^+_n</span> as in <a href="#eq-argminDkl" class="quarto-xref">Equation&nbsp;2</a>, but rather in the subset of matrices of the form <span class="math display"> \mathcal{L}_{n,k} = \left\{ \sum_{i=1}^k (\alpha_i-1) \frac{\mathbf{d}_i \mathbf{d}_i^\top}{\lVert \mathbf{d}_i \rVert^2} + I_n: \alpha_1, \ldots, \alpha_k &gt;0 \ \text{ and the $\mathbf{d}_i$'s are orthogonal} \right\}. </span> The relevant minimization problem thus becomes <span id="eq-argminDkl-k"><span class="math display">
    (\mathbf{m}^*_k, \mathbf{\Sigma}^*_k) = \arg\min \left\{ D(g^*,g_{\mathbf{m},\mathbf{\Sigma}}): \mathbf{m} \in \mathbb{R}^n, \ \mathbf{\Sigma} \in \mathcal{L}_{n,k} \right\}
     \tag{5}</span></span> instead of <a href="#eq-argminDkl" class="quarto-xref">Equation&nbsp;2</a>, with the effective dimension <span class="math inline">k</span> being allowed to be adjusted dynamically. By restricting the space in which the variance is assessed, one seeks to limit the number of variance terms to be estimated. The idea is that if the directions are suitably chosen, then the improvement of the accuracy due to the smaller error in estimating the variance terms will compensate the fact that we consider less candidates for the covariance matrix. In <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span>, the authors consider <span class="math inline">k = 1</span> and <span class="math inline">\mathbf{d}_1 = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert</span>. When <span class="math inline">f</span> is Gaussian, this choice is motivated by the fact that, due to the light tail of the Gaussian random variable and the reliability context, the variance should vary significantly in the direction of <span class="math inline">\mathbf{m}^*</span> and so estimating the variance in this direction can bring information. In <a href="#sec-mm" class="quarto-xref">Section&nbsp;3.5</a>, we use the techniques of the present paper to provide a stronger theoretical justification of this choice, see <a href="#thm-thm2" class="quarto-xref">Theorem&nbsp;2</a> and the discussion following it. The method in <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> is more involved: <span class="math inline">k</span> is adjusted dynamically, while the directions <span class="math inline">\mathbf{d}_i</span> are the eigenvectors associated to the largest eigenvalues of a certain matrix. They span a low-dimensional subspace called Failure-Informed Subspace, and the authors in <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> prove that this choice minimizes an upper bound on the minimal KL divergence. In practice, this algorithm yields very accurate results. However, we will not consider it further in the present paper for two reasons. First, this algorithm is tailored for the reliability case where <span class="math inline">\phi = \mathbb{I}_{\{\varphi \geq 0\}}</span>, with a function <span class="math inline">\varphi: \mathbb{R}^n \to \mathbb{R}</span>, whereas our method is more general and applies to the general problem of estimating an integral (see for instance our test case of <strong>?@sec-sub:payoff</strong>). Second, the algorithm in <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> requires the evaluation of the gradient of the function <span class="math inline">\varphi</span>. However, this gradient is not always known and can be expensive to evaluate in high dimensions; in some cases, the function <span class="math inline">\varphi</span> is even not differentiable, as will be the case in our numerical example in <strong>?@sec-sub:portfolio</strong>. In contrast, our method makes no assumption on the form or smoothness of <span class="math inline">\phi</span>: it does not need to assume that it is of the form <span class="math inline">\mathbb{I}_{\{\varphi \geq 0\}}</span>, or to assume that <span class="math inline">\nabla \varphi</span> is tractable. For completeness, whenever the algorithm of <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> was applicable and computing the gradient of <span class="math inline">\varphi</span> did not require any additional simulation budget, we have run it on the test cases considered here and found that it outperformed our algorithm. In more realistic settings, computing <span class="math inline">\nabla \varphi</span> would likely increase the simulation budget, and it would be interesting to compare the two algorithms in more details to understand when this extra computation cost is worthwhile. We reserve such a question for future research and will not consider the algorithm of <span class="citation" data-cites="UribeEtAl_CrossentropybasedImportanceSampling_2020">(<a href="#ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" role="doc-biblioref">Uribe et al. 2021</a>)</span> further, as our aim in this paper is to establish benchmark results for a general algorithm which works for any function <span class="math inline">\phi</span>.</p>
</section>
<section id="definition-of-the-function-ell" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="definition-of-the-function-ell"><span class="header-section-number">3.2</span> Definition of the function <span class="math inline">\ell</span></h2>
<p>The statement of our result involves the following function <span class="math inline">\ell</span>, which is represented in <a href="#fig-l" class="quarto-xref">Figure&nbsp;1</a>: <span id="eq-l"><span class="math display">
    \ell: x \in (0,\infty) \mapsto -\log(x) + x - 1.
     \tag{6}</span></span> In the following, <span class="math inline">(\lambda, \mathbf{d}) \in \mathbb{R} \times \mathbb{R}^n</span> is an eigenpair of a matrix <span class="math inline">A</span> if <span class="math inline">A\mathbf{d} = \lambda \mathbf{d}</span> and <span class="math inline">\lVert \mathbf{d} \rVert = 1</span>. A diagonalizable matrix has <span class="math inline">n</span> distinct eigenpairs, say <span class="math inline">((\lambda_i, \mathbf{d}_i), i = 1, \ldots, n)</span>, and we say that these eigenpairs are ranked in decreasing <span class="math inline">\ell</span>-order if <span class="math inline">\ell(\lambda_1) \geq \cdots \geq \ell(\lambda_n)</span>. In the rest of the article, we denote as <span class="math inline">(\lambda^*_i, \mathbf{d}^*_i)</span> the eigenpairs of <span class="math inline">\mathbf{\Sigma}^*</span> ranked in decreasing <span class="math inline">\ell</span>-order and as <span class="math inline">({\widehat{\lambda}}^*_i, \widehat{\mathbf{d}}^*_i)</span> the eigenpairs of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> ranked in decreasing <span class="math inline">\ell</span>-order.</p>
<div id="cell-fig-l" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 1. Plot of the function "l"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">### the following library is available on the following website : </span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">### "Papaioannou, I., Geyer, S., and Straub, D. (2019b). </span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">### Software tools for reliability analysis :</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">### Cross entropy method and improved cross entropy method. Retrieved from </span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">### https://www.cee.ed.tum.de/en/era/software/reliability/"</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CEIS_vMFNM <span class="im">import</span> <span class="op">*</span>      </span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display, Math, Latex</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tabulate <span class="im">import</span> tabulate</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(np.finfo(<span class="bu">float</span>).eps,<span class="fl">4.0</span>,<span class="dv">100</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="op">-</span>np.log(x) <span class="op">+</span> x <span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>ax.plot(x, y, linewidth<span class="op">=</span><span class="fl">2.0</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">4</span>), xticks<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>],</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>       ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), yticks<span class="op">=</span>[<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="fl">1.5</span>])</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$x$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(x)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-l" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-l-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-l-output-1.svg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-l-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Plot of the function <span class="math inline">\ell</span> given by <a href="#eq-l" class="quarto-xref">Equation&nbsp;6</a>.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-main-result-positioning" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-main-result-positioning"><span class="header-section-number">3.3</span> Main result of the paper</h2>
<p>The main result of the present paper is to compute the exact value for <span class="math inline">\mathbf{\mathbf{\Sigma}}^*_k</span> in <a href="#eq-argminDkl-k" class="quarto-xref">Equation&nbsp;5</a>, which therefore paves the way for efficient high-dimensional estimation schemes.</p>
<div id="thm-thm1" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1</strong></span> Let <span class="math inline">(\lambda^*_i, \mathbf{d}^*_i)</span> be the eigenpairs of <span class="math inline">\mathbf{\Sigma}^*</span> ranked in decreasing <span class="math inline">\ell</span>-order. Then for <span class="math inline">1 \leq k \leq n</span>, the solution <span class="math inline">(\mathbf{m}^*_k, \mathbf{\Sigma}^*_k)</span> to <a href="#eq-argminDkl-k" class="quarto-xref">Equation&nbsp;5</a> is given by <span id="eq-Sigma-k"><span class="math display">
\mathbf{m}^*_k = \mathbf{m}^* \ \text{ and } \ \mathbf{\Sigma}^*_k = I_n + \sum_{i=1}^k \left( \lambda^*_i - 1 \right) \mathbf{d}^*_i (\mathbf{d}^*_i)^\top.
\tag{7}</span></span></p>
</div>
<p>The proof of <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a> is detailed in <a href="#sec-proof">Appendix A</a>. For <span class="math inline">k = 1</span> for instance, the matrix <span class="math inline">\mathbf{\Sigma}^*_1 = I_n + (\lambda_1^*-1) \mathbf{d}_1^* (\mathbf{d}_1^*)^\top</span> with <span class="math inline">(\lambda_1^*, \mathbf{d}_1^*)</span> the eigenpair of <span class="math inline">\mathbf{\Sigma}^*</span> such as <span class="math inline">\lambda_1^*</span> is either the largest or the smallest eigenvalue of <span class="math inline">\mathbf{\Sigma}^*</span>, depending on which one maximizes <span class="math inline">\ell</span>.</p>
<p>This theoretical result therefore suggests to reduce dimension by computing the covariance matrix <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> and its eigenpairs, rank them in decreasing <span class="math inline">\ell</span>-order and then use the <span class="math inline">k</span> first eigenpairs <span class="math inline">(({\widehat{\lambda}}^*_i, {\widehat{\mathbf{d}}}^*_i), i = 1, \ldots, k)</span> to build the covariance matrix <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n</span> and the corresponding auxiliary density. This scheme is summarized in Algorithm 1. The effective dimension <span class="math inline">k</span> is obtained by Algorithm 2, see <a href="#sec-choicek" class="quarto-xref">Section&nbsp;3.4</a> below. The proof of the theorem is shown in <a href="#sec-proof">Appendix A</a>.</p>
<div class="pseudocode-container" data-alg-title="Algorithm" data-pseudocode-index="1">
<div class="pseudocode">
\begin{algorithm} \caption{Algorithm suggested by Theorem 1.} \begin{algorithmic} \State \textbf{Data}: Sample sizes $N$ and $M$ \State \textbf{Result}: Estimation $\widehat{\mathcal{E}_N}$ of integral $\mathcal{E}$ \State - Generate a sample $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$ on $\mathbb{R}^n$ independently according to $g^*$ \State - Estimate $\widehat{\mathbf{m}}^*$ and $\widehat{\mathbf{\Sigma}}^*$ defined in Equation 8 and Equation 9 with this sample \State - Compute the eigenpairs $(\widehat{\lambda}^*_i, \widehat{\mathbf{d}}^*_i)$ of $\widehat{\mathbf{\Sigma}}^*$ ranked in decreasing $\ell$-order \State - Compute the matrix $\widehat{\mathbf{\Sigma}}^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n$ with $k$ obtained by applying Algorithm 2 with input $({\widehat{\lambda}}^*_1, \ldots, {\widehat{\lambda}}^*_n)$ \State - Generate a new sample $\mathbf{X}_1,\ldots,\mathbf{X}_N$ independently from $g' = g_{\widehat{\mathbf{m}}^*,\widehat{\mathbf{\Sigma}}^*_k}$ \State - Return $\displaystyle \widehat{\mathcal{E}_N}=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}$ \end{algorithmic} \end{algorithm}
</div>
</div>
<div class="proof remark">
<p><span class="proof-title"><em>Remark</em>. </span>Since the function <span class="math inline">\ell</span> is minimized at 1, eigenpairs with <span class="math inline">\lambda^*_i =1</span> are selected in the sum of <a href="#eq-Sigma-k" class="quarto-xref">Equation&nbsp;7</a> once all other eigenpairs have been picked as the eigenpairs are <span class="math inline">\ell</span>-ordered: in other words, if <span class="math inline">\lambda^*_i = 1</span> then <span class="math inline">\lambda^*_j = 1</span> for all <span class="math inline">j \geq i</span>. Note also that the minimizer <span class="math inline">1</span> plays a special role as we are interested in covariance matrices of <span class="math inline">\mathcal{L}_{n,k}</span> which, once diagonalized, have mostly ones in the main diagonal (except for k values associated with the <span class="math inline">\alpha_i</span>). As <span class="math inline">k</span> will be small (See <a href="#sec-choicek" class="quarto-xref">Section&nbsp;3.4</a>), typically <span class="math inline">k = 1</span> or <span class="math inline">2</span>, this amounts to finding covariance matrices that are perturbations of the identity (this is relevant as we assume <span class="math inline">f</span> is standard Gaussian). Therefore, when approximating <span class="math inline">\mathbf{\Sigma}^*</span> by such matrices, we should first consider eigenvalues as different as possible from <span class="math inline">1</span> (with the discrepancy from 1 being measured by <span class="math inline">\ell</span>).</p>
</div>
<p>In the first step of Algorithm 1, we assume <span class="math inline">g^*</span> can be sampled independently. This is a reasonable assumption as classical techniques such as importance sampling with self-normalized weights or Markov Chain Monte Carlo (MCMC) can be applied in this case (see for instance <span class="citation" data-cites="ChanKroese_ImprovedCrossentropyMethod_2012">(<a href="#ref-ChanKroese_ImprovedCrossentropyMethod_2012" role="doc-biblioref">Chan and Kroese 2012</a>)</span>, <span class="citation" data-cites="GraceEtAl_AutomatedStateDependentImportance_2014">(<a href="#ref-GraceEtAl_AutomatedStateDependentImportance_2014" role="doc-biblioref">Grace, Kroese, and Sandmann 2014</a>)</span>). In this paper, we choose to apply a basic rejection method that yields perfect independent samples from <span class="math inline">g^*</span>, possibly at the price of a high computational cost. As the primary goal of this paper is to understand whether the <span class="math inline">\mathbf{d}^*_i</span>’s are indeed good projection directions, this cost will not be taken into account. Possible improvements to relax this assumption are discussed in the conclusion of the paper and in <a href="#sec-MCMC">Appendix C</a>.</p>
</section>
<section id="sec-choicek" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-choicek"><span class="header-section-number">3.4</span> Choice of the number of dimensions <span class="math inline">k</span></h2>
<p>The choice of the effective dimension <span class="math inline">k</span>, i.e., the number of projection directions considered, is important. If it is close to <span class="math inline">n</span>, then the matrix <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> will be close to <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> which is the situation we want to avoid in the first place. On the other hand, setting <span class="math inline">k=1</span> in all cases may be too simple and lead to suboptimal results. In practice, however this is often a good choice. In order to adapt <span class="math inline">k</span> dynamically, we consider a simple method based on the value of the KL divergence. Given the eigenvalues <span class="math inline">\lambda_1, \ldots, \lambda_n</span> ranked in decreasing <span class="math inline">\ell</span>-order, we look for the maximal gap between two consecutive eigenvalues of the sequence <span class="math inline">(\ell(\lambda_1), \ldots, \ell(\lambda_n))</span>. This allows to choose <span class="math inline">k</span> such that <span class="math inline">\sum_{i=1}^k \ell(\lambda_i)</span> is close to <span class="math inline">\sum_{i=1}^n \ell(\lambda_i)</span> which is equal, up to an additive constant, to the minimal KL divergence (shown in <strong>?@lem-D</strong>). The precise method is described in Algorithm 2.</p>
<div class="pseudocode-container" data-alg-title="Algorithm" data-pseudocode-index="2">
<div class="pseudocode">
\begin{algorithm} \caption{Choice of the number of dimensions} \begin{algorithmic} \State \textbf{Data}: Sequence of positive numbers $\lambda_1, \ldots, \lambda_n$ in decreasing $\ell$-order \State \textbf{Result}: Number of selected dimensions $k$ \State - Compute the increments $\delta_i = \ell(\lambda_{i+1}) - \ell(\lambda_i)$ for $i=1\ldots n-1$ \State - Return $k=\arg\max \delta_i$, the index of the maximum of the differences. \end{algorithmic} \end{algorithm}
</div>
</div>
</section>
<section id="sec-mm" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="sec-mm"><span class="header-section-number">3.5</span> Theoretical result concerning the projection on <span class="math inline">\mathbf{m}^*</span></h2>
<p>In <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span>, the authors propose to project on the mean <span class="math inline">\mathbf{m}^*</span> of the optimal auxiliary density <span class="math inline">g^*</span>. Numerically, this algorithm is shown to perform well, but only a very heuristic explanation based on the light tail of the Gaussian distribution is provided to motivate this choice. It turns out that the techniques used in the proof of <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a> can shed light on why projecting on <span class="math inline">\mathbf{m}^*</span> may indeed be a good idea. Let us first state our theoretical result, and then explain why it justifies the idea of projecting on <span class="math inline">\mathbf{m}^*</span>.</p>
<div id="thm-thm2" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2</strong></span> Consider <span class="math inline">\mathbf{\Sigma} \in \mathcal{L}_{n,1}</span> of the form <span class="math inline">\mathbf{\Sigma} = I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top</span> with <span class="math inline">\alpha &gt; 0</span> and <span class="math inline">\lVert \mathbf{d} \rVert = 1</span>. Then the minimizer in <span class="math inline">(\alpha, \mathbf{d})</span> of the KL divergence between <span class="math inline">f</span> and <span class="math inline">g_{\mathbf{m}^*, \mathbf{\Sigma}}</span> is <span class="math inline">(1+\lVert \mathbf{m}^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert)</span>: <span class="math display">\left( 1+\lVert \mathbf{m}^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert \right) = \arg \min_{\alpha, \mathbf{d}} \left\{ D(f, g_{\mathbf{m}^*, I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top}): \alpha &gt; 0, \ \lVert \mathbf{d} \rVert = 1 \right\}. </span></p>
</div>
<p>The proof of <a href="#thm-thm2" class="quarto-xref">Theorem&nbsp;2</a> is detailed in <a href="#sec-proof">Appendix A</a>. In other words, <span class="math inline">\mathbf{m}^*</span> appears as an optimal projection direction when one seeks to minimize the KL divergence between <span class="math inline">f</span> and the Gaussian density with mean <span class="math inline">\mathbf{m}^*</span> and covariance of the form <span class="math inline">I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top</span>. Let us now explain why this minimization problem is indeed relevant, and why choosing an auxiliary density which minimizes this KL divergence may indeed lead to an accurate estimation. The justification deeply relies on the recent results by <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span>.</p>
<p>As mentioned above, in a reliability context where one seeks to estimate a small probability <span class="math inline">p = \mathbb{P}(\mathbf{X} \in A),</span> Theorem <span class="math inline">1.3</span> in <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span> shows that <span class="math inline">D(g^*, g)</span> governs the sample size required for an accurate estimation of <span class="math inline">p</span>: more precisely, the estimation is accurate if the sample size is larger than <span class="math inline">e^{D(g^*, g)}</span>, and inaccurate otherwise. This motivates the rationale for minimizing the KL divergence with <span class="math inline">g^*</span>.</p>
<p>However, in high dimensions, importance sampling is known to fail because of the weight degeneracy problem whereby <span class="math inline">\max_i L_i / \sum_i L_i \approx 1</span>, with the <span class="math inline">L_i</span>’s the unnormalized importance weights, or likelihood ratios: <span class="math inline">L_i = f(\mathbf{X}_i) / g(\mathbf{X}_i)</span> with the <span class="math inline">\mathbf{X}_i</span>’s i.i.d. drawn according to <span class="math inline">g</span>. Theorem <span class="math inline">2.3</span> in <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span> shows that the weight degeneracy problem is avoided if the empirical mean of the likelihood ratios is close to <span class="math inline">1</span>, and for this, Theorem <span class="math inline">1.1</span> in <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span> shows that the sample size should be larger than <span class="math inline">e^{D(f, g)}</span>. In other words, these results suggest that the KL divergence with <span class="math inline">g^*</span> governs the sample size for an accurate estimation of <span class="math inline">p</span>, while the KL divergence with <span class="math inline">f</span> governs the weight degeneracy problem.</p>
<p>In light of these results, it becomes natural to consider the KL divergence with <span class="math inline">f</span> and not only <span class="math inline">g^*</span> <span class="citation" data-cites="OwenZhou_SafeEffectiveImportance_2000">(<a href="#ref-OwenZhou_SafeEffectiveImportance_2000" role="doc-biblioref">Owen and Zhou 2000</a>)</span>. Of course, minimizing <span class="math inline">D(f, g_{\mathbf{m}, \mathbf{\Sigma}})</span> without constraints on <span class="math inline">\mathbf{m}</span> and <span class="math inline">\mathbf{\Sigma}</span> is trivial since <span class="math inline">g_{\mathbf{m}, \mathbf{\Sigma}} = f</span> for <span class="math inline">\mathbf{m} = 0</span> and <span class="math inline">\mathbf{\Sigma} = I_n</span>. However, these choices are the ones we want to avoid in the first place, and so it makes sense to impose some constraints on <span class="math inline">\mathbf{m}</span> and <span class="math inline">\mathbf{\Sigma}</span>. If one keeps in mind the other objective of getting close to <span class="math inline">g^*</span>, then the choice <span class="math inline">\mathbf{m} = \mathbf{m}^*</span> becomes very natural, and we are led to considering the optimization problem of <a href="#thm-thm2" class="quarto-xref">Theorem&nbsp;2</a> (when <span class="math inline">\mathbf{\Sigma} \in \mathcal{L}_{n,1}</span> is a rank-1 perturbation of the identity).</p>
</section>
</section>
<section id="sec-num-results-framework" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Computational framework</h1>
<section id="sec-def_proc" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="sec-def_proc"><span class="header-section-number">4.1</span> Numerical procedure for IS estimate comparison</h2>
<p>The objective of the numerical simulations is to evaluate the impact of the choice of the covariance matrix on the estimation accuracy of a high dimensional integral <span class="math inline">\mathcal{E}</span>. We thus want to compare the IS estimation results for different auxiliary densities and more particularly for different choices of the auxiliary covariance matrix when the IS auxiliary density is Gaussian. The details of the considered covariance matrices is given in <a href="#sec-def_cov" class="quarto-xref">Section&nbsp;4.2</a>. To extend this comparison, we also compute the results when the IS auxiliary density is chosen with the von Mises–Fisher–Nakagami (vMFN) model recently proposed in <span class="citation" data-cites="PapaioannouEtAl_ImprovedCrossEntropybased_2019">(<a href="#ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" role="doc-biblioref">Papaioannou, Geyer, and Straub 2019</a>)</span> for high dimensional probability estimation (See <a href="#sec-naka">Appendix B</a>).</p>
<p>In <a href="#sec-test-cases" class="quarto-xref">Section&nbsp;5</a> we test these different models of auxiliary densities on five test cases, where <span class="math inline">f</span> is a standard Gaussian density. This choice is not a theoretical limitation as we can in principle always come back to this case by transforming the vector <span class="math inline">\mathbf{X}</span> with isoprobabilistic transformations (see for instance <span class="citation" data-cites="HohenbichlerRackwitz_NonNormalDependentVectors_1981">(<a href="#ref-HohenbichlerRackwitz_NonNormalDependentVectors_1981" role="doc-biblioref">Hohenbichler and Rackwitz 1981</a>)</span>, <span class="citation" data-cites="LiuDerKiureghian_MultivariateDistributionModels_1986">(<a href="#ref-LiuDerKiureghian_MultivariateDistributionModels_1986" role="doc-biblioref">Liu and Der Kiureghian 1986</a>)</span>).</p>
<p>The precise numerical framework that we will consider to assess the efficiency of the different auxiliary models is as follows. We assume first that <span class="math inline">M</span> i.i.d.&nbsp;random samples <span class="math inline">\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M</span> distributed from <span class="math inline">g^*</span> are available from rejection sampling (unless in <a href="#sec-MCMC">Appendix C</a> where we consider MCMC). From these samples, the parameters of the Gaussian and of the vMFN auxiliary density are computed to get an auxiliary density <span class="math inline">g'</span>. Finally, <span class="math inline">N</span> samples are generated from <span class="math inline">g'</span> to provide an estimation of <span class="math inline">\mathcal{E}</span> with IS. This procedure is summarized by the following stages:</p>
<ol type="1">
<li>Generate a sample <span class="math inline">\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M</span> independently according to <span class="math inline">g^*</span>;</li>
<li>From <span class="math inline">\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M</span>, compute the parameters of the auxiliary parametric density <span class="math inline">g'</span>;</li>
<li>Generate a new sample <span class="math inline">\mathbf{X}_1,\ldots,\mathbf{X}_N</span> independently from <span class="math inline">g'</span>;</li>
<li>Estimate <span class="math inline">\mathcal{E}</span> with <span class="math inline">\widehat{\mathcal{E}_N}=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}</span>.</li>
</ol>
<p>The number of samples <span class="math inline">M</span> and <span class="math inline">N</span> are respectively set to <span class="math inline">M=500</span> and <span class="math inline">N=2000</span>. The computational cost to generate <span class="math inline">M=500</span> samples distributed from <span class="math inline">g^*</span> with rejection sampling is often unaffordable in practice; if <span class="math inline">\mathcal{E}</span> is a probability of order <span class="math inline">10^{-p}</span>, then approximately <span class="math inline">500\times10^p</span> calls to <span class="math inline">\phi</span> are necessary for the generation of <span class="math inline">\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M</span>. Finally, whatever the auxiliary parametric density <span class="math inline">g'</span> computed from <span class="math inline">\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M</span>, the number of calls to <span class="math inline">\phi</span> for the estimation step stays constant and equal to <span class="math inline">N</span>. The number of calls to <span class="math inline">\phi</span> for the whole procedure on a <span class="math inline">10^{-p}</span> probability estimation is about <span class="math inline">500\times10^p+N</span>. A more realistic situation is considered in <a href="#sec-MCMC">Appendix C</a> where MCMC is applied to generate samples from <span class="math inline">g^*</span>. The resulting samples are dependent but the computational cost is significanlty reduced. The number of calls to <span class="math inline">\phi</span> with MCMC is then equal to <span class="math inline">M</span> which leads to a total computational cost of <span class="math inline">M+N</span> for the whole procedure.</p>
<p>This procedure is then repeated <span class="math inline">500</span> times to provide a mean estimation <span class="math inline">\widehat{\mathcal{E}}</span> of <span class="math inline">\mathcal{E}</span>. In the result tables, for each auxiliary density <span class="math inline">g'</span> we report the corresponding value for the relative error <span class="math inline">\widehat{\mathcal{E}}/ \mathcal{E}-1</span> and the coefficient of variation of the <span class="math inline">500</span> iterations (the empirical standard deviation divided by <span class="math inline">\mathcal{E}</span>). As was established in the proof of <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a>, the KL divergence is, up to an additive constant, equal to <span class="math inline">D'(\mathbf{\Sigma}) = \log \lvert \mathbf{\Sigma} \rvert + \textrm{tr}(\mathbf{\Sigma}^* \mathbf{\Sigma}^{-1})</span> which we will refer to as partial KL divergence. In the result tables, we also report thus the mean value of <span class="math inline">D'(\mathbf{\Sigma})</span> to analyse the relevance of the auxiliary density <span class="math inline">g_{\widehat{\mathbf{m}}^*, \mathbf{\Sigma}}</span> for six choices of covariance matrix <span class="math inline">\mathbf{\Sigma}</span>. The next sections specify the different parameters of <span class="math inline">g'</span> for the Gaussian model and for the vMFN model we have considered in the simulations.</p>
</section>
<section id="sec-def_cov" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="sec-def_cov"><span class="header-section-number">4.2</span> Choice of the auxiliary density <span class="math inline">g'</span> for the Gaussian model</h2>
<p>The goal is to get benchmark results to assess whether one can improve estimations of Gaussian IS auxiliary density by projecting the covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span> in the proposed directions <span class="math inline">\mathbf{d}^*_i</span>. The algorithm that we study here (Algorithms 1+2) aims more precisely at understanding whether:</p>
<ul>
<li>projecting can improve the situation with respect to the empirical covariance matrix;</li>
<li>the <span class="math inline">\mathbf{d}^*_i</span>’s are good candidates, in particular compared to the choice <span class="math inline">\mathbf{m}^*</span> suggested in <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span>;</li>
<li>what is the impact in making errors in estimating the eigenpairs <span class="math inline">(\lambda^*_i, \mathbf{d}^*_i)</span>.</li>
</ul>
<p>Let us define the estimate <span class="math inline">\widehat{\mathbf{m}}^*</span> of <span class="math inline">\mathbf{m}^*</span> from the <span class="math inline">M</span> i.i.d. random samples <span class="math inline">\mathbf{X}_1^*,\ldots,\mathbf{X}_M^*</span> distributed from <span class="math inline">g^*</span> with <span id="eq-hatm"><span class="math display">
    \widehat{\mathbf{m}}^* = \frac{1}{M}\sum_{i=1}^M \mathbf{X}_i^*.
\tag{8}</span></span> In our numerical test cases, we will compare six different choices of Gaussian auxiliary distributions <span class="math inline">g'</span> with mean <span class="math inline">\widehat{\mathbf{m}}^*</span> and the following covariance matrices summarized in <a href="#tbl-sigma" class="quarto-xref">Table&nbsp;1</a>:</p>
<ol type="1">
<li><p><span class="math inline">\mathbf{\Sigma}^*</span>: the optimal covariance matrix given by <a href="#eq-mstar" class="quarto-xref">Equation&nbsp;3</a>;</p></li>
<li><p><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>: the empirical estimation of <span class="math inline">\mathbf{\Sigma}^*</span> given by <span id="eq-hatSigma"><span class="math display">
\widehat{\mathbf{\Sigma}}^* = \frac{1}{M}\sum_{i=1}^M (\mathbf{X}_i^*-\widehat{\mathbf{m}}^*)(\mathbf{X}_i^*-\widehat{\mathbf{m}}^*)^\top.
\tag{9}</span></span></p></li>
</ol>
<p>The four other covariance matrices considered in the numerical simulations are of the form <span class="math inline">\sum_{i=1}^k (v_i-1) \mathbf{d}_i \mathbf{d}^\top_i + I_n</span> where <span class="math inline">v_i</span> is the variance of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> in the direction <span class="math inline">\mathbf{d}_i</span>, <span class="math inline">v_i = \mathbf{d}_i^\top \widehat{\mathbf{\Sigma}}^* \mathbf{d}_i</span>. The considered choice of <span class="math inline">k</span> and <span class="math inline">\mathbf{d}_i</span> gives the following covariance matrices:</p>
<ol start="3" type="1">
<li><p><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> is obtained by choosing <span class="math inline">\mathbf{d}_i = \mathbf{d}^*_i</span> of <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a>, which is supposed to be perfectly known from <span class="math inline">\mathbf{\Sigma}^*</span> and <span class="math inline">k</span> is computed with Algorithm 2;</p></li>
<li><p><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span> is obtained by choosing <span class="math inline">\mathbf{d}_i = {\widehat{\mathbf{d}}}^*_i</span> the <span class="math inline">i</span>-th eigenvector of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (in <span class="math inline">\ell</span>-order), which is an estimation of <span class="math inline">\mathbf{d}^*_i</span>, and <span class="math inline">k</span> is computed with Algorithm 2;</p></li>
<li><p><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> is obtained by choosing <span class="math inline">k = 1</span> and <span class="math inline">\mathbf{d}_1 = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert</span>;</p></li>
<li><p><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> is obtained by choosing <span class="math inline">k = 1</span> and <span class="math inline">\mathbf{d}_1 = {\widehat{\mathbf{m}}}^* / \lVert {\widehat{\mathbf{m}}}^* \rVert</span>, where <span class="math inline">\widehat{\mathbf{m}}^*</span> given by <a href="#eq-hatm" class="quarto-xref">Equation&nbsp;8</a>.</p></li>
</ol>
<p>The matrices <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> use the estimation <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> with the optimal directions <span class="math inline">\mathbf{d}^*_i</span> or <span class="math inline">\mathbf{m}^*</span>, while the matrices <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> involve an estimation of these directions from <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>. By definition, <span class="math inline">\mathbf{\Sigma}^*</span> will give optimal results, while results for <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> will deteriorate as the dimension increases, which is the well-known behavior which we try to improve. Moreover, <span class="math inline">\mathbf{\Sigma}^*</span> and the projection directions <span class="math inline">\mathbf{d}^*_i</span> or <span class="math inline">\mathbf{m}^*</span>, are of course unknown in practice. For simulation comparison purpose, they could be determined analytically in simple test cases and otherwise we obtained them by a brute force Monte Carlo scheme with a very high simulation budget. Finally, we emphasize that Algorithm 1 corresponds to estimating and projecting on the <span class="math inline">\mathbf{d}^*_i</span>’s, and so the matrix <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> of Algorithm 1 is equal to the matrix <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span>.</p>
<div id="tbl-sigma" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sigma-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Presentation of the six covariance matrices considered in the numerical examples.
</figcaption>
<div aria-describedby="tbl-sigma-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\mathbf{\Sigma}^*</span></th>
<th><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></th>
<th><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span></th>
<th><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span></th>
<th><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span></th>
<th><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Initial covariance matrix</td>
<td><span class="math inline">\mathbf{\Sigma}^*</span></td>
<td><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></td>
<td><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></td>
<td><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></td>
<td><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></td>
<td><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span></td>
<td></td>
</tr>
<tr class="even">
<td>Projection directions (exact or estimated)</td>
<td>-</td>
<td>-</td>
<td>Exact</td>
<td>Exact</td>
<td>Estimated</td>
<td>Estimated</td>
<td></td>
</tr>
<tr class="odd">
<td>Choice for the projection direction</td>
<td>None</td>
<td>None</td>
<td>Opt</td>
<td>Mean</td>
<td>Opt</td>
<td>Mean</td>
<td></td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
</section>
<section id="sec-test-cases" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Numerical results on five test cases</h1>
<p>The proposed numerical framework is applied on three examples that are often considered to assess the performance of importance sampling algorithms and also two test cases from the area of financial mathematics.</p>
<section id="sec-sub:sum" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-sub:sum"><span class="header-section-number">5.1</span> Test case 1: one-dimensional optimal projection</h2>
<p>We consider a test case where all computations can be made exactly. This is a classical example of rare event probability estimation, often used to test the robustness of a method in high dimensions. It is given by <span class="math inline">\phi(\mathbf{x})=\mathbb{I}_{\{\varphi(\mathbf{x})\geq 0\}}</span> with <span class="math inline">\varphi</span> the following affine function: <span id="eq-sum"><span class="math display">
    \varphi: \mathbf{x}=(x_1,\ldots,x_n)\in\mathbb{R}^n \mapsto\underset{j=1}{\overset{n}{\sum}} x_j-3\sqrt{n}.
\tag{10}</span></span> The quantity of interest <span class="math inline">\mathcal{E}</span> is defined as <span class="math inline">\mathcal{E}=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)\simeq 1.35\cdot 10^{-3}</span> for all <span class="math inline">n</span> where the density <span class="math inline">f</span> is the standard <span class="math inline">n</span>-dimensional Gaussian distribution. Here, the zero-variance density is <span class="math inline">g^*(\mathbf{x})=\dfrac{f(\mathbf{x})\mathbb{I}_{\{\varphi(\mathbf{x})\geq 0\}}}{\mathcal{E}}</span>, and the optimal parameters <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{\Sigma}^*</span> in <a href="#eq-mstar" class="quarto-xref">Equation&nbsp;3</a> can be computed exactly, namely <span class="math inline">\mathbf{m}^* = \alpha \textbf{1}</span> with <span class="math inline">\alpha = e^{-9/2}/(\mathcal{E}(2\pi)^{1/2})</span> and <span class="math inline">\textbf{1} = \frac{1}{\sqrt n} (1,\ldots,1) \in \mathbb{R}^n</span> the normalized constant vector, and <span class="math inline">\mathbf{\Sigma}^* =(v-1) \mathbf{1} \mathbf{1}^\top + I_n</span> with <span class="math inline">v=3\alpha-\alpha^2+1</span>.</p>
<section id="evolution-of-the-partial-kl-divergence-and-spectrum" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="evolution-of-the-partial-kl-divergence-and-spectrum"><span class="header-section-number">5.1.1</span> Evolution of the partial KL divergence and spectrum</h3>
<p><a href="#fig-eigsum-1" class="quarto-xref">Figure&nbsp;2 (a)</a> represents the evolution as the dimension varies between <span class="math inline">5</span> and <span class="math inline">100</span> of the partial KL divergence <span class="math inline">D'</span> for three different choices of covariance matrix: the optimal matrix <span class="math inline">\mathbf{\Sigma}^*</span>, its empirical estimation <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> and the estimation <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> of the optimal lower-dimensional covariance matrix. We can notice that the partial KL divergence for <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> grows much faster than the other two, and that the partial KL divergence for <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> remains very close to the optimal value <span class="math inline">D'(\mathbf{\Sigma}^*)</span>. As the KL divergence is a proxy for the efficiency of the auxiliary density (it is for instance closely related to the number of samples required for a given precision <span class="citation" data-cites="Chatterjee18:0">(<a href="#ref-Chatterjee18:0" role="doc-biblioref">Chatterjee and Diaconis 2018</a>)</span>), this suggests that using <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> will provide results close to optimal.</p>
<p>We now check this claim. As <span class="math inline">\mathbf{\Sigma}^* = (v-1) \textbf{1} \textbf{1}^\top + I_n</span>, its eigenpairs are <span class="math inline">(v, \textbf{1})</span> and <span class="math inline">(1,\mathbf{d}_i)</span> where the <span class="math inline">\mathbf{d}_i</span>’s form an orthonormal basis of the space orthogonal to the space spanned by <span class="math inline">\textbf{1}</span>. In particular, <span class="math inline">(v, \textbf{1})</span> is the largest (in <span class="math inline">\ell</span>-order) eigenpair of <span class="math inline">\mathbf{\Sigma}^*</span> and <span class="math inline">\mathbf{\Sigma}^*_k = \mathbf{\Sigma}^*</span> for any <span class="math inline">k \geq 1</span>.</p>
<p>In practice, we do not use this theoretical knowledge and <span class="math inline">\mathbf{\Sigma}^*</span>, <span class="math inline">\mathbf{\Sigma}^*_k</span> and the eigenpairs are estimated. The six covariance matrices introduced in <a href="#sec-def_cov" class="quarto-xref">Section&nbsp;4.2</a> and in which we are interested are as follows:</p>
<ul>
<li><span class="math inline">\mathbf{\Sigma}^* = (v-1) \textbf{1} \textbf{1}^\top + I_n</span>;</li>
<li><span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> given by <a href="#eq-hatSigma" class="quarto-xref">Equation&nbsp;9</a>;</li>
<li><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> are equal and given by <span class="math inline">(\widehat \lambda-1) \textbf{1} \textbf{1}^\top + I_n</span> with <span class="math inline">\widehat{\lambda} = \textbf{1}^\top \widehat{\mathbf{\Sigma}}^* \textbf{1}</span>. This amounts to assuming that the projection direction <span class="math inline">\textbf{1}</span> is perfectly known, whereas the variance in this direction is estimated;</li>
<li><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}} = (\widehat{\lambda} - 1) \widehat{\mathbf{d}} {\widehat{\mathbf{d}}}^\top + I_n</span> with <span class="math inline">(\widehat{\lambda}, \widehat{\mathbf{d}})</span> the smallest eigenpair of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>. The difference with the previous case is that we do not assume anymore that the optimal projection direction <span class="math inline">\textbf{1}</span> is known, and so it needs to be estimated;</li>
<li><span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}} = (\widehat{\lambda} - 1) \frac{\widehat{\mathbf{m}}^* {(\widehat{\mathbf{m}}^*)}^\top}{\lVert \widehat{\mathbf{m}}^* \rVert^2} + I_n</span> with <span class="math inline">\widehat{\mathbf{m}}^*</span> given by <a href="#eq-hatm" class="quarto-xref">Equation&nbsp;8</a> and <span class="math inline">\widehat{\lambda} = \frac{{(\widehat{\mathbf{m}}^*)}^\top \widehat{\mathbf{\Sigma}}^* \widehat{\mathbf{m}}^*}{\lVert \widehat{\mathbf{m}}^* \rVert^2}</span>. Here we assume that <span class="math inline">\mathbf{m}^*</span> is a good projection direction, but is unknown and therefore needs to be estimated.</li>
</ul>
<p>Note that in the particularly simple case considered here, both <span class="math inline">\widehat{\mathbf{m}}^* / \lVert \widehat{\mathbf{m}}^* \rVert</span> and <span class="math inline">\widehat{\mathbf{d}}</span> are estimators of <span class="math inline">\textbf{1}</span> but they are obtained by different methods. In the next example we will consider a case where <span class="math inline">\mathbf{m}^*</span> is not an optimal projection direction as given by <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a>.</p>
<p><a href="#fig-eigsum-2" class="quarto-xref">Figure&nbsp;2 (b)</a> represents the images by <span class="math inline">\ell</span> of the eigenvalues of <span class="math inline">\mathbf{\Sigma}^*</span> and <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>. This picture carries a very important insight. We notice that the estimation of most eigenvalues is poor: indeed, all the blue crosses except the leftmost one are meant to be estimator of <span class="math inline">1</span>, whereas we see that they are more or less uniformly spread around <span class="math inline">1</span>. This means that the variance terms in the corresponding directions are poorly estimated, which could be the explanation on why the use of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> gives an inaccurate estimation. But what we remark also is that the function <span class="math inline">\ell</span> is quite flat around one: as a consequence, although the eigenvalues offer significant variability, this variability is smoothed by the action of <span class="math inline">\ell</span>. Indeed, the images of the eigenvalues by <span class="math inline">\ell</span> take values between <span class="math inline">0</span> and <span class="math inline">0.8</span> and have smaller variability. Moreover, <span class="math inline">\ell(x)</span> increases sharply as <span class="math inline">x</span> approaches <span class="math inline">0</span> and thus efficiently distinguishes between the two leftmost estimated eigenvalues and is able to separate them.</p>
<div class="cell" data-layout="[[45,-10,45],[45,-10,45]]" data-execution_count="2">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 2. Evolution of the partial KL divergence and spectrum of the</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the test case 1</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Somme(x):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(x)[<span class="dv">1</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(np.<span class="bu">sum</span>(x,axis<span class="op">=</span><span class="dv">1</span>)<span class="op">-</span><span class="dv">3</span><span class="op">*</span>np.sqrt(n))</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Somme</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span>sp.stats.norm.cdf(<span class="op">-</span><span class="dv">3</span>)   <span class="co"># exact value of the integral</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mstar</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span>np.exp(<span class="op">-</span><span class="dv">3</span><span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>(E<span class="op">*</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi))</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    Mstar<span class="op">=</span>alpha<span class="op">*</span>np.ones(d)<span class="op">/</span>np.sqrt(d)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sigmastar</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    vstar<span class="op">=</span><span class="dv">3</span><span class="op">*</span>alpha<span class="op">-</span>alpha<span class="op">**</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>    Sigstar<span class="op">=</span> (vstar<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>np.ones((d,d))<span class="op">/</span>d<span class="op">+</span>np.eye(d)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">## g*-sample</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>np.eye(d))</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X0[ind,:]</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[:M,:]            <span class="co"># g*-sample of size M</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                  </span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># matrix of inflential directions of projection</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)   </span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d)  </span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>                    Sigstar.dot(np.linalg.inv(sigma))))</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(<span class="op">\</span></span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>                    np.diag(Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d</span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-eigsum" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-eigsum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-eigsum" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-eigsum-1" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-eigsum-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-eigsum-output-1.svg" class="img-fluid figure-img" data-ref-parent="fig-eigsum">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-eigsum-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix <span class="math inline">\mathbf{\Sigma}^*</span> (red squares), the sample covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue circles), and the projected covariance <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span> (black dots).
</figcaption>
</figure>
</div>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 10.0%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-eigsum" style="flex-basis: 45.0%;justify-content: flex-start;">
<div id="fig-eigsum-2" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-eigsum-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="published-elmasri-optimal_files/figure-html/fig-eigsum-output-2.svg" class="img-fluid figure-img" data-ref-parent="fig-eigsum">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-eigsum-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Computation of <span class="math inline">\ell(\lambda_i)</span> for the eigenvalues of <span class="math inline">\mathbf{\Sigma}^*</span> (red squares) and <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> (blue crosses) in dimension <span class="math inline">n = 100</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-eigsum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Partial KL divergence and spectrum for the function <span class="math inline">\phi = \mathbb{I}_{\varphi \geq 0}</span> with <span class="math inline">\varphi</span> the linear function given by <a href="#eq-sum" class="quarto-xref">Equation&nbsp;10</a>.
</figcaption>
</figure>
</div>
</div>
</section>
<section id="numerical-results" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="numerical-results"><span class="header-section-number">5.1.2</span> Numerical results</h3>
<p>We report in <a href="#tbl-sum" class="quarto-xref">Table&nbsp;2</a> the numerical results for the six different matrices and the vMFN model for the dimension <span class="math inline">n=100</span>. The column <span class="math inline">\mathbf{\Sigma}^*</span> gives the optimal results, while the column <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> corresponds to the results that we are trying to improve. Comparing these two columns, we notice as expected that the estimation of <span class="math inline">\mathcal{E}</span> with <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> is significantly degraded. Compared to the first column <span class="math inline">\mathbf{\Sigma}^*</span>, the third and fourth columns with <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}} =  {\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> correspond to the best projection direction <span class="math inline">\textbf{1}</span> (as for <span class="math inline">\mathbf{\Sigma}^*</span>) but estimating the variance in this direction (instead of the true variance) with <span class="math inline">\textbf{1}^\top \widehat{\mathbf{\Sigma}}^* \textbf{1}</span>. This choice performs very well, with numerical results similar to the optimal ones. This can be understood since in this case, both <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> and <span class="math inline">\mathbf{\Sigma}^*</span> are of the form <span class="math inline">\alpha \textbf{1} \textbf{1}^\top + I_n</span> and so estimating <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> requires only a one-dimensional estimation (namely, the estimation of <span class="math inline">\alpha</span>). Next, the last two columns <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> highlight the impact of having to estimate the projection directions in addition to the variance since these two matrices are of the form <span class="math inline">\widehat \alpha \widehat{\textbf{1}} {\widehat{\textbf{1}}}^\top + I_n</span> with both <span class="math inline">\widehat{\alpha}</span> (the variance term) and <span class="math inline">\widehat{\textbf{1}}</span> (the direction) being estimated. We observe that these matrices yield results which are close to optimal and greatly improve the estimation obtained using <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>.</p>
<p>Moreover, we observe that <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span> gives better results than <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}</span>. We suggest that this is because <span class="math inline">\widehat{\mathbf{m}}^* / \lVert \widehat{\mathbf{m}}^* \rVert</span> is a better estimator of <span class="math inline">\textbf{1}</span> than the eigenvector of <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span>. Indeed, evaluating <span class="math inline">\widehat{\mathbf{m}}^*</span> requires the estimation of <span class="math inline">n</span> parameters, whereas <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> needs around <span class="math inline">n^2/2</span> parameters to estimate, so the eigenvector is finally more noisy than the mean vector. In the last column, we present the vMFN estimation that is slightly more efficicent than the estimation obtained with <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}</span>.</p>
<p>Thus, the proposed idea improves significantly the probability estimation in high dimensions. But we see that the method taken in <span class="citation" data-cites="MasriEtAl_ImprovementCrossentropyMethod_2020">(<a href="#ref-MasriEtAl_ImprovementCrossentropyMethod_2020" role="doc-biblioref">El Masri, Morio, and Simatos 2021</a>)</span> with the projection <span class="math inline">\mathbf{m}^*</span> is at least as much efficient in this example where we need only a one-dimensional projection. The next case shows that the projection on more than one direction can outperform the one-dimensional projection on <span class="math inline">\mathbf{m}^*</span>.</p>
<div class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Hide/Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 2. Numerical comparison on test case 1</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Somme</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>Tabresult <span class="op">=</span> np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>,</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_\text</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>  tablefmt<span class="op">=</span><span class="st">"latex"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-sum" class="cell quarto-float anchored" data-execution_count="3">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-sum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Numerical comparison of the estimation of <span class="math inline">\mathcal{E} \approx 1.35\cdot 10^{-3}</span> considering the Gaussian model with the six covariance matrices defined in <a href="#sec-def_cov" class="quarto-xref">Section&nbsp;4.2</a> and the vFMN model, when <span class="math inline">\phi = \mathbb{I}_{{\varphi\geq 0}}</span> with <span class="math inline">\varphi</span> the linear function given by <a href="#eq-sum" class="quarto-xref">Equation&nbsp;10</a>. As explained in the text, <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> are actually equal in this case. The computational cost is <span class="math inline">N=2000</span>.
</figcaption>
<div aria-describedby="tbl-sum-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="3">

</div>
</div>
</figure>
</div>
</div>
</section>
</section>
<section id="sec-sub:parabol" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="sec-sub:parabol"><span class="header-section-number">5.2</span> Test case 2: projection in 2 directions</h2>
<p>The second test case is again a probability estimation, i.e., it is of the form <span class="math inline">\phi = \mathbb{I}_{\{\varphi \geq 0\}}</span> with now the function <span class="math inline">\varphi</span> having some quadratic terms: <span id="eq-parabol"><span class="math display">
    \varphi: \mathbf{x}=(x_1,\ldots,x_n) \in \mathbb{R}^n \mapsto x_1 - 25 x_2^2 - 30 x_3^2 - 1.
\tag{11}</span></span> The quantity of interest <span class="math inline">\mathcal{E}</span> is defined as <span class="math inline">\mathcal{E}=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)</span> for all <span class="math inline">n</span> where the density <span class="math inline">f</span> is the standard <span class="math inline">n</span>-dimensional Gaussian distribution. This function is motivated in part because <span class="math inline">\mathbf{m}^*</span> and <span class="math inline">\mathbf{d}^*_1</span> are different and also because Algorithm 2 chooses two projection directions. Thus, this is an example where <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}</span> and <span class="math inline">{\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}</span> are significantly different.</p>
<section id="evolution-of-the-partial-kl-divergence-and-spectrum-1" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="evolution-of-the-partial-kl-divergence-and-spectrum-1"><span class="header-section-number">5.2.1</span> Evolution of the partial KL divergence and spectrum</h3>
<p>We check on <strong>?@fig-inefficiency-parab-1</strong> that the partial KL divergence obeys the same behavior as for the previous example, namely the one associated with <span class="math inline">\widehat{\mathbf{\Sigma}}^*</span> increases much faster than the ones associated with <span class="math inline">\mathbf{\Sigma}^*</span> and <span class="math inline">\widehat{\mathbf{\Sigma}}^*_k</span>, which again suggests that projecting can improve the situation. Since the function <span class="math inline">\varphi</span> only depends on the first three variables and is even in <span class="math inline">x_2</span> and <span class="math inline">x_3</span>, one gets that <span class="math inline">\mathbf{m}^* = \alpha
    \textbf{e}_1</span> with <span class="math inline">\alpha = \mathbb{E}(X_1 \mid X_1 \geq 25 X^2_2 + 30 X^2_3 + 1) \approx 1.9</span> (here and in the sequel, <span class="math inline">\textbf{e}_i</span> denotes the <span class="math inline">i</span>th canonical vector of <span class="math inline">\mathbb{R}^n</span>, i.e., all its coordinates are <span class="math inline">0</span> except the <span class="math inline">i</span>-th one which is equal to one), and that <span class="math inline">\mathbf{\Sigma}^*</span> is diagonal with <span class="math display"> \mathbf{\Sigma}^* =
    \begin{pmatrix}
    \lambda_1 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
    0 &amp; \lambda_2 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
    0 &amp; 0 &amp; \lambda_3 &amp; 0 &amp; \cdots &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\
    \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\
    \end{pmatrix}.
</span> Note that the off-diagonal elements of the submatrix <span class="math inline">(\mathbf{\Sigma}^*_{ij})_{1 \leq i, j \leq 3}</span> are indeed <span class="math inline">0</span> since they result from integrating an odd function of an odd random variable with an even conditioning. For instance, if <span class="math inline">F(x) = \mathbb{P}(30 X^2_3 + 1 \leq x)</span>, then by conditioning on <span class="math inline">(X_1, X_3)</span> we obtain <span class="math display">
    \mathbf{\Sigma}^*_{12} = \mathbb{E} \left( (X_1 - \alpha) X_2 \mid X_1 - 25 X_2^2 \geq 30 X^2_3 + 1 \right)\\
     = \frac{1}{\mathcal{E}} \mathbb{E} \left[ (X_1 - \alpha) \mathbb{E} \left( X_2 F(X_1 - 25 X^2_2) \mid X_1 \right) \right]
</span> which is <span class="math inline">0</span> as <span class="math inline">x_2 F(x_1 - x^2_2)</span> is an odd function of <span class="math inline">x_2</span> for fixed <span class="math inline">x_1</span>, and <span class="math inline">X_2</span> has an even density.</p>
<p>We can numerically compute <span class="math inline">\lambda_1 \approx 0.28</span>, <span class="math inline">\lambda_2 \approx 0.009</span> and <span class="math inline">\lambda_3 \approx 0.008</span>. These values correspond to the red squares in <strong>?@fig-inefficiency-parab-2</strong> which shows that the smallest eigenvalues are properly estimated. Moreover, Algorithm 2 selects the two largest eigenvalues, which have the highest <span class="math inline">\ell</span>-values. These two eigenvalues thus correspond to the eigenvectors <span class="math inline">\mathbf{e}_2</span> and <span class="math inline">\mathbf{e}_3</span>, and so we see that on this example, the optimal directions predicted by <a href="#thm-thm1" class="quarto-xref">Theorem&nbsp;1</a> are significantly different (actually, orthogonal) from <span class="math inline">\mathbf{m}^*</span> which is proportional to <span class="math inline">\textbf{e}_1</span>.</p>
</section>
</section>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-AgapiouEtAl_ImportanceSamplingIntrinsic_2017" class="csl-entry" role="listitem">
Agapiou, Sergios, Omiros Papaspiliopoulos, Daniel Sanz-Alonso, and Andrew M Stuart. 2017. <span>“Importance <span>Sampling</span> : <span>Intrinsic Dimension</span> and <span>Computational Cost</span>.”</span> <em>Statistical Science</em> 32 (3): 405–31. <a href="https://doi.org/10.1214/17-STS611">https://doi.org/10.1214/17-STS611</a>.
</div>
<div id="ref-AshurbekovaEtAl_OptimalShrinkageRobust_" class="csl-entry" role="listitem">
Ashurbekova, Karina, Antoine Usseglio-Carleve, Florence Forbes, and Sophie Achard. 2020. <span>“Optimal Shrinkage for Robust Covariance Matrix Estimators in a Small Sample Size Setting.”</span>
</div>
<div id="ref-AuBeck_ImportantSamplingHigh_2003" class="csl-entry" role="listitem">
Au, S. K., and J. L. Beck. 2003. <span>“Important Sampling in High Dimensions.”</span> <em>Structural Safety</em> 25 (2): 139–63. <a href="https://doi.org/10.1016/S0167-4730(02)00047-4">https://doi.org/10.1016/S0167-4730(02)00047-4</a>.
</div>
<div id="ref-BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008" class="csl-entry" role="listitem">
Bengtsson, Thomas, Peter Bickel, and Bo Li. 2008. <span>“Curse-of-Dimensionality Revisited: <span>Collapse</span> of the Particle Filter in Very Large Scale Systems.”</span> In <em>Institute of <span>Mathematical Statistics Collections</span></em>, 316–34. <span>Beachwood, Ohio, USA</span>: <span>Institute of Mathematical Statistics</span>. <a href="https://doi.org/10.1214/193940307000000518">https://doi.org/10.1214/193940307000000518</a>.
</div>
<div id="ref-bucklew2013introduction" class="csl-entry" role="listitem">
Bucklew, James. 2013. <span>“Introduction to Rare Event Simulation.”</span> In, 58–61. Springer Science &amp; Business Media. <a href="https://doi.org/10.1007/978-1-4757-4078-3">https://doi.org/10.1007/978-1-4757-4078-3</a>.
</div>
<div id="ref-BugalloEtAl_AdaptiveImportanceSampling_2017" class="csl-entry" role="listitem">
Bugallo, Monica F., Victor Elvira, Luca Martino, David Luengo, Joaquin Miguez, and Petar M. Djuric. 2017. <span>“Adaptive <span>Importance Sampling</span>: <span>The</span> Past, the Present, and the Future.”</span> <em>IEEE Signal Processing Magazine</em> 34 (4): 60–79. <a href="https://doi.org/10.1109/MSP.2017.2699226">https://doi.org/10.1109/MSP.2017.2699226</a>.
</div>
<div id="ref-CappeEtAl_AdaptiveImportanceSampling_2008" class="csl-entry" role="listitem">
Cappé, Olivier, Randal Douc, Arnaud Guillin, Jean-Michel Marin, and Christian P. Robert. 2008. <span>“Adaptive Importance Sampling in General Mixture Classes.”</span> <em>Statistics and Computing</em> 18 (4): 447–59. <a href="https://doi.org/10.1007/s11222-008-9059-x">https://doi.org/10.1007/s11222-008-9059-x</a>.
</div>
<div id="ref-ChanKroese_ImprovedCrossentropyMethod_2012" class="csl-entry" role="listitem">
Chan, Joshua C. C., and Dirk P. Kroese. 2012. <span>“Improved Cross-Entropy Method for Estimation.”</span> <em>Statistics and Computing</em> 22 (5): 1031–40. <a href="https://doi.org/10.1007/s11222-011-9275-7">https://doi.org/10.1007/s11222-011-9275-7</a>.
</div>
<div id="ref-Chatterjee18:0" class="csl-entry" role="listitem">
Chatterjee, Sourav, and Persi Diaconis. 2018. <span>“The Sample Size Required in Importance Sampling.”</span> <em>The Annals of Applied Probability</em> 28 (2): 1099–1135. <a href="https://doi.org/10.1214/17-AAP1326">https://doi.org/10.1214/17-AAP1326</a>.
</div>
<div id="ref-CornuetEtAl_AdaptiveMultipleImportance_2012" class="csl-entry" role="listitem">
Cornuet, Jean-Marie, Jean-Michel Marin, Antonietta Mira, and Christian P. Robert. 2012. <span>“Adaptive Multiple Importance Sampling.”</span> <em>Scandinavian Journal of Statistics</em> 39 (4): 798–812. <a href="https://doi.org/10.1111/j.1467-9469.2011.00756.x">https://doi.org/10.1111/j.1467-9469.2011.00756.x</a>.
</div>
<div id="ref-MasriEtAl_ImprovementCrossentropyMethod_2020" class="csl-entry" role="listitem">
El Masri, Maxime, Jérôme Morio, and Florian Simatos. 2021. <span>“Improvement of the Cross-Entropy Method in High Dimension for Failure Probability Estimation Through a One-Dimensional Projection Without Gradient Estimation.”</span> <em>Reliability Engineering &amp; System Safety</em> 216: 107991. <a href="https://doi.org/10.1016/j.ress.2021.107991">https://doi.org/10.1016/j.ress.2021.107991</a>.
</div>
<div id="ref-El-LahamEtAl_RecursiveShrinkageCovariance_" class="csl-entry" role="listitem">
El-Laham, Yousef, Vı́ctor Elvira, and Mónica Bugallo. 2019. <span>“Recursive Shrinkage Covariance Learning in Adaptive Importance Sampling.”</span> In <em>2019 IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)</em>, 624–28. IEEE. <a href="https://doi.org/10.1109/CAMSAP45676.2019.9022450">https://doi.org/10.1109/CAMSAP45676.2019.9022450</a>.
</div>
<div id="ref-fan2008high" class="csl-entry" role="listitem">
Fan, Jianqing, Yingying Fan, and Jinchi Lv. 2008. <span>“High Dimensional Covariance Matrix Estimation Using a Factor Model.”</span> <em>Journal of Econometrics</em> 147 (1): 186–97.
</div>
<div id="ref-GraceEtAl_AutomatedStateDependentImportance_2014" class="csl-entry" role="listitem">
Grace, Adam W., Dirk P. Kroese, and Werner Sandmann. 2014. <span>“Automated <span>State</span>-<span>Dependent Importance Sampling</span> for <span>Markov Jump Processes</span> via <span>Sampling</span> from the <span>Zero</span>-<span>Variance Distribution</span>.”</span> <em>Journal of Applied Probability</em> 51 (3): 741–55. <a href="https://doi.org/10.1239/jap/1409932671">https://doi.org/10.1239/jap/1409932671</a>.
</div>
<div id="ref-HohenbichlerRackwitz_NonNormalDependentVectors_1981" class="csl-entry" role="listitem">
Hohenbichler, Michael, and Rüdiger Rackwitz. 1981. <span>“Non-<span>Normal Dependent Vectors</span> in <span>Structural Safety</span>.”</span> <em>Journal of the Engineering Mechanics Division</em> 107 (6): 1227–38. <a href="https://doi.org/10.1061/JMCEA3.0002777">https://doi.org/10.1061/JMCEA3.0002777</a>.
</div>
<div id="ref-LedoitWolf_WellconditionedEstimatorLargedimensional_2004" class="csl-entry" role="listitem">
Ledoit, Olivier, and Michael Wolf. 2004. <span>“A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices.”</span> <em>Journal of Multivariate Analysis</em> 88 (2): 365–411. <a href="https://doi.org/10.1016/S0047-259X(03)00096-4">https://doi.org/10.1016/S0047-259X(03)00096-4</a>.
</div>
<div id="ref-LiuDerKiureghian_MultivariateDistributionModels_1986" class="csl-entry" role="listitem">
Liu, Pei-Ling, and Armen Der Kiureghian. 1986. <span>“Multivariate Distribution Models with Prescribed Marginals and Covariances.”</span> <em>Probabilistic Engineering Mechanics</em> 1 (2): 105–12. <a href="https://doi.org/10.1016/0266-8920(86)90033-0">https://doi.org/10.1016/0266-8920(86)90033-0</a>.
</div>
<div id="ref-OwenZhou_SafeEffectiveImportance_2000" class="csl-entry" role="listitem">
Owen, Art, and Yi Zhou. 2000. <span>“Safe and <span>Effective Importance Sampling</span>.”</span> <em>Journal of the American Statistical Association</em> 95 (449): 135–43. <a href="https://doi.org/10.1080/01621459.2000.10473909">https://doi.org/10.1080/01621459.2000.10473909</a>.
</div>
<div id="ref-PapaioannouEtAl_ImprovedCrossEntropybased_2019" class="csl-entry" role="listitem">
Papaioannou, Iason, Sebastian Geyer, and Daniel Straub. 2019. <span>“Improved Cross Entropy-Based Importance Sampling with a Flexible Mixture Model.”</span> <em>Reliability Engineering &amp; System Safety</em> 191 (November): 106564. <a href="https://doi.org/10.1016/j.ress.2019.106564">https://doi.org/10.1016/j.ress.2019.106564</a>.
</div>
<div id="ref-RubinsteinKroese_CrossentropyMethodUnified_2011" class="csl-entry" role="listitem">
Rubinstein, Reuven Y., and Dirk P Kroese. 2011a. <em>The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, <span>Monte</span>-<span>Carlo</span> Simulation and Machine Learning</em>. <span>New York; London</span>: <span>Springer</span>. <a href="https://doi.org/10.1007/978-1-4757-4321-0">https://doi.org/10.1007/978-1-4757-4321-0</a>.
</div>
<div id="ref-RubinsteinKroese_CrossentropyMethodUnified_2011v2" class="csl-entry" role="listitem">
———. 2011b. <span>“The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, <span>Monte</span>-<span>Carlo</span> Simulation and Machine Learning.”</span> In, 67–72. <span>New York; London</span>: <span>Springer</span>. <a href="https://doi.org/10.1007/978-1-4757-4321-0">https://doi.org/10.1007/978-1-4757-4321-0</a>.
</div>
<div id="ref-RubinsteinKroese_SimulationMonteCarlo_2017v2" class="csl-entry" role="listitem">
Rubinstein, Reuven Y., and Dirk P. Kroese. 2017. <span>“Simulation and the <span>Monte Carlo</span> Method.”</span> In, Third edition, 149–58. Wiley Series in Probability and Statistics. <span>Hoboken, New Jersey</span>: <span>Wiley</span>. <a href="https://doi.org/10.1002/9781118631980">https://doi.org/10.1002/9781118631980</a>.
</div>
<div id="ref-UribeEtAl_CrossentropybasedImportanceSampling_2020" class="csl-entry" role="listitem">
Uribe, Felipe, Iason Papaioannou, Youssef M. Marzouk, and Daniel Straub. 2021. <span>“Cross-Entropy-Based Importance Sampling with Failure-Informed Dimension Reduction for Rare Event Simulation.”</span> <em>SIAM/ASA Journal on Uncertainty Quantification</em> 9 (2): 818–47. <a href="https://doi.org/10.1137/20M1344585">https://doi.org/10.1137/20M1344585</a>.
</div>
</div>
<!-- -->

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a></div></div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@article{el masri2024,
  author = {El Masri, Maxime and Morio, Jérôme and Simatos, Florian},
  publisher = {French Statistical Society},
  title = {BUG {Optimal} Projection for Parametric Importance Sampling
    in High Dimensions},
  journal = {Computo},
  date = {2024-11-03},
  url = {https://computo.sfds.asso.fr/published-202402-elmasri-optimal/},
  doi = {10.57750/jjza-6j82},
  issn = {2824-7795},
  langid = {en},
  abstract = {We propose a dimension reduction strategy in order to
    improve the performance of importance sampling in high dimensions.
    The idea is to estimate variance terms in a small number of suitably
    chosen directions. We first prove that the optimal directions, i.e.,
    the ones that minimize the Kullback-\/-Leibler divergence with the
    optimal auxiliary density, are the eigenvectors associated with
    extreme (small or large) eigenvalues of the optimal covariance
    matrix. We then perform extensive numerical experiments showing that
    as dimension increases, these directions give estimations which are
    very close to optimal. Moreover, we demonstrate that the estimation
    remains accurate even when a simple empirical estimator of the
    covariance matrix is used to compute these directions. The
    theoretical and numerical results open the way for different
    generalizations, in particular the incorporation of such ideas in
    adaptive importance sampling schemes.}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-el masri2024" class="csl-entry quarto-appendix-citeas" role="listitem">
El Masri, Maxime, Jérôme Morio, and Florian Simatos. 2024. <span>“BUG
Optimal Projection for Parametric Importance Sampling in High
Dimensions.”</span> <em>Computo</em>, November. <a href="https://doi.org/10.57750/jjza-6j82">https://doi.org/10.57750/jjza-6j82</a>.
</div></div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb4" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> BUG Optimal projection for parametric importance sampling in high dimensions</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Maxime El Masri</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: '[ONERA/DTIS](https://www.onera.fr/), [ISAE-SUPAERO](https://www.isae-supaero.fr/), [Université de Toulouse](https://www.univ-toulouse.fr/)'</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-9127-4503</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Jérôme Morio</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">    url: 'https://www.onera.fr/en/staff/jerome-morio?destination=node/981'</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: '[ONERA/DTIS](https://www.onera.fr/), [Université de Toulouse](https://www.univ-toulouse.fr/)'</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0002-8811-8956</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Florian Simatos</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co">    url: 'https://pagespro.isae-supaero.fr/florian-simatos/'</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliation: '[ISAE-SUPAERO](https://www.isae-supaero.fr/), [Université de Toulouse](https://www.univ-toulouse.fr/)'</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> |</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">  This document provides a dimension-reduction strategy in order to improve the performance of importance sampling in high dimensions.</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> |</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">  We propose a dimension reduction strategy in order to improve the performance of importance sampling in high dimensions. The idea is to estimate variance terms in a small number of suitably chosen directions. We first prove that the optimal directions, i.e., the ones that minimize the Kullback--Leibler divergence with the optimal auxiliary density, are the eigenvectors associated with extreme (small or large) eigenvalues of the optimal covariance matrix. We then perform extensive numerical experiments showing that as dimension increases, these directions give estimations which are very close to optimal. Moreover, we demonstrate that the estimation remains accurate even when a simple empirical estimator of the covariance matrix is used to compute these directions. The theoretical and numerical results open the way for different generalizations, in particular the incorporation of such ideas in adaptive importance sampling schemes.</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="an">keywords:</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">  - Rare event simulation</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co">  - Parameter estimation</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co">  - Importance sampling</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co">  - Dimension reduction</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co">  - Kullback--Leibler divergence</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co">  - Projection</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="an">github-user:</span><span class="co"> computo</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="an">repo:</span><span class="co"> optimal-projection-IS</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 11/03/2024</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="an">published:</span><span class="co"> false</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="an">google-scholar:</span><span class="co"> false</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a><span class="an">citation:</span></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a><span class="co">  type: article-journal</span></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a><span class="co">  container-title: "Computo"</span></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a><span class="co">  doi: "10.57750/jjza-6j82"</span></span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a><span class="co">  publisher: "French Statistical Society"</span></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a><span class="co">  issn: "2824-7795"</span></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a><span class="co">  pdf-url: "https://computo.sfds.asso.fr/published-202402-elmasri-optimal/published-202312-elmasri-optimal.pdf"</span></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a><span class="co">  url:  "https://computo.sfds.asso.fr/published-202402-elmasri-optimal/"</span></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-html: default</span></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a><span class="co">  computo-pdf: default</span></span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span></span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="co">  keep-ipynb: true</span></span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span></span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a><span class="co">  jupytext:</span></span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a><span class="co">    text_representation:</span></span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a><span class="co">      extension: .qmd</span></span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a><span class="co">      format_name: quarto</span></span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a><span class="co">      format_version: '1.0'</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a><span class="co">      jupytext_version: 1.14.2</span></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a><span class="co">  kernelspec:</span></span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a><span class="co">    display_name: Python 3 (ipykernel)</span></span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a><span class="co">    language: python</span></span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a><span class="co">    name: python3</span></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction  </span></span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>Importance Sampling (IS) is a stochastic method to estimate integrals of the form $\mathcal{E} = \int \phi(\mathbf{x})f(\mathbf{x})\textrm{d} \mathbf{x}$ with a black-box function $\phi$ and a probability density function (pdf) $f$. It rests upon the choice of an auxiliary density which can significantly improve the estimation compared to the naive Monte Carlo (MC) method <span class="co">[</span><span class="ot">@AgapiouEtAl_ImportanceSamplingIntrinsic_2017</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@OwenZhou_SafeEffectiveImportance_2000</span><span class="co">]</span>. The theoretical optimal IS density, also called zero-variance density, is defined by $\phi f / \mathcal{E}$ when $\phi$ is a positive function. This density is not available in practice as it involves the unknown integral $\mathcal{E}$, but a classical strategy consists in searching for an optimal approximation in a parametric family of densities. By minimising a "distance" to the optimal IS density, such as the Kullback--Leibler divergence, one can find optimal parameters in this family to get an efficient sampling pdf. Adaptive Importance Sampling (AIS) algorithms, such as the Mixture Population Monte Carlo method <span class="co">[</span><span class="ot">@CappeEtAl_AdaptiveImportanceSampling_2008</span><span class="co">]</span>, the Adaptive Multiple Importance Sampling method <span class="co">[</span><span class="ot">@CornuetEtAl_AdaptiveMultipleImportance_2012</span><span class="co">]</span>, or the Cross Entropy method <span class="co">[</span><span class="ot">@RubinsteinKroese_CrossentropyMethodUnified_2011</span><span class="co">]</span>, estimate the optimal parameters adaptively by updating at intermediate levels <span class="co">[</span><span class="ot">@BugalloEtAl_AdaptiveImportanceSampling_2017</span><span class="co">]</span>.</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>These techniques work very well, but only for moderate dimensions. In high dimensions, most of these techniques fail to give suitable parameters for two reasons: </span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>the weight degeneracy problem, for which the self-normalized likelihood ratios (weights) in the IS densities degenerate in the sense that the largest one takes all the mass, while all other weights are negligible so that the final estimation essentially uses only one sample. See for instance <span class="co">[</span><span class="ot">@BengtssonEtAl_CurseofdimensionalityRevisitedCollapse_2008</span><span class="co">]</span> for a theoretical analysis in the related context of particle filtering. The conditions under which importance sampling is applicable in high dimensions are notably investigated in a reliability context in <span class="co">[</span><span class="ot">@AuBeck_ImportantSamplingHigh_2003</span><span class="co">]</span>: it is remarked that the optimal covariance matrix should not deviate significantly from the identity matrix. <span class="co">[</span><span class="ot">@El-LahamEtAl_RecursiveShrinkageCovariance_</span><span class="co">]</span> tackle the weight degeneracy problem by applying a recursive shrinkage of the covariance matrix, which is constructed iteratively with a weighted sum of the sample covariance estimator and a biased, but more stable, estimator;</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>the intricate estimation of distribution parameters in high dimensions and particularly covariance matrices, whose size increases quadratically in the dimension <span class="co">[</span><span class="ot">@AshurbekovaEtAl_OptimalShrinkageRobust_</span><span class="co">]</span>,<span class="co">[</span><span class="ot">@LedoitWolf_WellconditionedEstimatorLargedimensional_2004</span><span class="co">]</span>. Empirical covariance matrix estimate has notably a slow convergence rate in high dimensions <span class="co">[</span><span class="ot">@fan2008high</span><span class="co">]</span>. For that purpose, dimension reduction techniques can be applied. The idea was recently put forth to reduce the effective dimension by only estimating these parameters (in particular the covariance matrix) in suitable directions <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span>. In this paper we delve deeper into this idea.</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>The main contribution of the present paper is to identify the optimal directions in the fundamental case when the parametric family is Gaussian, and perform numerical simulations in order to understand how they behave in practice. In particular, we propose directions which, in contrast to the recent paper <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span>, do not require the objective function to be differentiable, and moreover optimizes the Kullback--Leibler distance with the optimal density instead of simply an upper bound on it, as in <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span>. In @sec-proj we elaborate in more details on the differences between the two approaches.</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>The paper is organised as follows: in @sec-IS we recall the foundations of IS. In @sec-main-result, we state our main theoretical result and we compare it with the current state-of-the-art. The proof of our theoretical result are given in Appendix; @sec-num-results-framework introduces the numerical framework that we have adopted, and @sec-test-cases presents the numerical results obtained on five different test cases to assess the efficiency of the directions that we propose. We conclude in @sec-Ccl with a summary and research perspectives. </span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a><span class="fu"># Importance Sampling {#sec-IS}</span></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>We consider the problem of estimating the following integral:</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>    \mathcal{E}=\mathbb{E}_f(\phi(\mathbf{X}))=\int \phi(\mathbf{x})f(\mathbf{x})\textrm{d} \mathbf{x},</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>where $\mathbf{X}$ is a random vector in $\mathbb{R}^n$ with standard Gaussian pdf $f$, and $\phi: \mathbb{R}^n\rightarrow\mathbb{R}_+$ is a real-valued, non-negative function. The function $\phi$ is considered as a black-box function which is potentially expensive to evaluate, and this means that the number of calls to $\phi$ should be limited.</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>IS is an approach used to reduce the variance of the classical Monte Carlo estimator of $\mathcal{E}$. The idea of IS is to generate a random sample $\mathbf{X}_1,\ldots,\mathbf{X}_N$ from an auxiliary density $g$, instead of $f$, and to compute the following estimator: </span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>    \widehat{\mathcal{E}_N}=\frac{1}{N}\sum_{i=1}^N \phi(\mathbf{X}_i)L(\mathbf{X}_i),</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-hatE}</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>    with $L=f/g$ the likelihood ratio, or importance weight, and the auxiliary density $g$, also called importance sampling density, is such that $g(\mathbf{x})=0$ implies $\phi(\mathbf{x}) f(\mathbf{x})=0$ for every $\mathbf{x}$ (which makes the product $\phi L$ well-defined). This estimator is consistent and unbiased but its accuracy strongly depends on the choice of the auxiliary density $g$. It is well known that the optimal choice for $g$ is <span class="co">[</span><span class="ot">@bucklew2013introduction</span><span class="co">]</span></span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a>    g^*(\mathbf{x})=\dfrac{\phi(\mathbf{x})f(\mathbf{x})}{\mathcal{E}}, \ \mathbf{x}\in\mathbb{R}^n.</span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>Indeed, for this choice we have $\phi L = \mathcal{E}$ and so $\widehat{\mathcal{E}}_N$ is actually the deterministic estimator $\mathcal{E}$. For this reason, $g^*$ is sometimes called zero-variance density, a terminology that we will adopt here. Of course, $g^*$ is only of theoretical interest as it depends on the unknown integral $\mathcal{E}$. However, it gives an idea of good choices for the auxiliary density $g$, and we will seek to approximate $g^*$ by an auxiliary density that minimizes a distance between $g^*$ and a given parametric family of densities.</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>In this paper, the parametric family of densities is the Gaussian family $<span class="sc">\{</span>g_{\mathbf{m}, \mathbf{\Sigma}}: \mathbf{m} \in \mathbb{R}^n, \mathbf{\Sigma} \in \mathcal{S}^+_n\}$, where $g_{\mathbf{m}, \mathbf{\Sigma}}$ denotes the Gaussian density with mean $\mathbf{m} \in \mathbb{R}^n$ and covariance matrix $\mathbf{\Sigma} \in \mathcal{S}^+_n$ with $\mathcal{S}^+_n \subset \mathbb{R}^{n \times n}$ the set of symmetric, positive-definite matrices:</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>    g_{\mathbf{m},\mathbf{\Sigma}}(\mathbf{x})=\dfrac{1}{ (2\pi)^{n/2} \lvert \mathbf{\Sigma} \rvert^{1/2}} \exp\left(-\frac{1}{2}(\mathbf{x}-\mathbf{m})^\top\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{m})\right), \ \mathbf{x} \in \mathbb{R}^n.</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>with $\lvert \mathbf{\Sigma} \rvert$ the determinant of $\mathbf{\Sigma}$. Moreover, we will consider the Kullback--Leibler (KL) divergence to measure a "distance" between $g^*$ and  $g_{\mathbf{m}, \mathbf{\Sigma}}$. Recall that for two densities $f$ and $h$, with $f$ absolutely continuous with respect to $h$, the KL divergence $D(f,h)$ between $f$ and $h$ is defined by: </span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>    D(f,h)=\mathbb{E}_{f}\left<span class="co">[</span><span class="ot">\log \left( \frac{f(\mathbf{X})}{h(\mathbf{X})} \right) \right</span><span class="co">]</span> = \int \log \left( \frac{f(\mathbf{x})}{h(\mathbf{x})} \right)f(\mathbf{x}) \textrm{d} \mathbf{x}.</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>Thus, our goal is to approximate $g^*$ by $g_{\mathbf{m}^*, \mathbf{\Sigma}^*}$ with the optimal mean vector $\mathbf{m}^*$ and the optimal covariance matrix $\mathbf{\Sigma}^*$ given by:</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>    (\mathbf{m}^*,\mathbf{\Sigma}^*) = \arg\min \left\{ D(g^*,g_{\mathbf{m},\mathbf{\Sigma}}): \mathbf{m} \in \mathbb{R}^n, \mathbf{\Sigma} \in \mathcal{S}_n^+ \right<span class="sc">\}</span>.</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-argminDkl}</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>This optimization is in general convex and differentiable with respect to $\mathbf{m}$ and $\mathbf{\Sigma}$. Moreover, the solution of @eq-argminDkl can be computed analytically by cancelling the gradient. In the Gaussian case, it is thus proved that $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$ are simply the mean and variance of the zero-variance density <span class="co">[</span><span class="ot">@RubinsteinKroese_CrossentropyMethodUnified_2011v2</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@RubinsteinKroese_SimulationMonteCarlo_2017v2</span><span class="co">]</span>:</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>    \mathbf{m}^*=\mathbb{E}_{g^*}(\mathbf{X}) \hspace{0.5cm} \text{ and } \hspace{0.5cm} \mathbf{\Sigma}^* = \textrm{Var}_{g^*} \left(\mathbf{X}\right).</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-mstar}</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a><span class="fu"># Efficient dimension reduction {#sec-main-result} </span></span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a><span class="fu">## Projecting onto a low-dimensional subspace {#sec-proj} </span></span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>As $g^*$ is unknown, the optimal parameters $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$ given by @eq-mstar are not directly computable. However, we can sample from the optimal density as it is known up to a multiplicative constant.</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>Therefore, usual estimation schemes start with estimating $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$, say through $\widehat{\mathbf{m}}^*$ and $\widehat{\mathbf{\Sigma}}^*$, respectively, and then use these approximations to estimate $\mathcal{E}$ through @eq-hatE with the auxiliary density $g_{\widehat{\mathbf{m}}^*, \widehat{\mathbf{\Sigma}}^*}$. Although the estimation of $\mathcal{E}$ with the auxiliary density $g_{\mathbf{m}^*, \mathbf{\Sigma}^*}$ usually provides very good results, it is well-known that in high dimensions, the additional error induced by the estimations of $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$ severely degrades the accuracy of the final estimation [@PapaioannouEtAl_ImprovedCrossEntropybased_2019], [@UribeEtAl_CrossentropybasedImportanceSampling_2020]. The main problem lies in the estimation of $\mathbf{\Sigma}^*$ which, in dimension $n$, involves the estimation of a quadratic (in the dimension) number of terms, namely $n(n+1)/2$.</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>Recently, the idea to overcome this problem by only evaluating variance terms in a small number of influential directions was explored in <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span>. In these two papers, the auxiliary covariance matrix $\mathbf{\Sigma}$ is modeled in the form</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>    \mathbf{\Sigma} = \sum_{i=1}^k (v_i-1) \mathbf{d}_i \mathbf{d}_i^\top + I_n</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-Sigmak}</span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>where the $\mathbf{d}_i$'s are the $k$ orthonormal directions which are deemed influential. It is easy to check that $\mathbf{\Sigma}$ is the covariance matrix of the Gaussian vector</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>    $$ v^{1/2}_1 Y_1 \mathbf{d}_1 + \cdots + v^{1/2}_k Y_k \mathbf{d}_k + Y_{k+1} \mathbf{d}_{k+1} + \cdots + Y_n \mathbf{d}_n $$</span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>where the $Y_i$'s are i.i.d. standard normal random variables (one-dimensional), and the $n-k$ vectors $(\mathbf{d}_{k+1}, \ldots, \mathbf{d}_n)$ complete $(\mathbf{d}_1, \ldots, \mathbf{d}_k)$ into an orthonormal basis. In particular, $v_i$ is the variance in the direction of $\mathbf{d}_i$, i.e., $v_i = \mathbf{d}_i^\top \mathbf{\Sigma} \mathbf{d}_i$. In @eq-Sigmak, $k$ can be considered as the effective dimension in which variance terms are estimated. In other words, in <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span> and <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span>, the optimal variance parameter is not sought in $\mathcal{S}^+_n$ as in @eq-argminDkl, but rather in the subset of matrices of the form</span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>    $$ \mathcal{L}_{n,k} = \left\{ \sum_{i=1}^k (\alpha_i-1) \frac{\mathbf{d}_i \mathbf{d}_i^\top}{\lVert \mathbf{d}_i \rVert^2} + I_n: \alpha_1, \ldots, \alpha_k &gt;0 \ \text{ and the $\mathbf{d}_i$'s are orthogonal} \right<span class="sc">\}</span>. $$</span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>The relevant minimization problem thus becomes</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>    $$ </span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>    (\mathbf{m}^*_k, \mathbf{\Sigma}^*_k) = \arg\min \left\{ D(g^*,g_{\mathbf{m},\mathbf{\Sigma}}): \mathbf{m} \in \mathbb{R}^n, \ \mathbf{\Sigma} \in \mathcal{L}_{n,k} \right<span class="sc">\}</span></span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-argminDkl-k}</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>instead of @eq-argminDkl, with the effective dimension $k$ being allowed to be adjusted dynamically. By restricting the space in which the variance is assessed, one seeks to limit the number of variance terms to be estimated. The idea is that if the directions are suitably chosen, then the improvement of the accuracy due to the smaller error in estimating the variance terms will compensate the fact that we consider less candidates for the covariance matrix.</span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>In <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span>, the authors consider $k = 1$ and $\mathbf{d}_1 = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert$. When $f$ is Gaussian, this choice is motivated by the fact that, due to the light tail of the Gaussian random variable and the reliability context, the variance should vary significantly in the direction of $\mathbf{m}^*$ and so estimating the variance in this direction can bring information. In @sec-mm, we use the techniques of the present paper to provide a stronger theoretical justification of this choice, see @thm-thm2 and the discussion following it. The method in [@UribeEtAl_CrossentropybasedImportanceSampling_2020] is more involved: $k$ is adjusted dynamically, while the directions $\mathbf{d}_i$ are the eigenvectors associated to the largest eigenvalues of a certain matrix. They span a low-dimensional subspace called Failure-Informed Subspace, and the authors in [@UribeEtAl_CrossentropybasedImportanceSampling_2020] prove that this choice minimizes an upper bound on the minimal KL divergence. In practice, this algorithm yields very accurate results. However, we will not consider it further in the present paper for two reasons. First, this algorithm is tailored for the reliability case where $\phi = \mathbb{I}_{<span class="sc">\{</span>\varphi \geq 0<span class="sc">\}</span>}$, with a function $\varphi: \mathbb{R}^n \to \mathbb{R}$, whereas our method is more general and applies to the general problem of estimating an integral (see for instance our test case of @sec-sub:payoff). Second, the algorithm in <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span> requires the evaluation of the gradient of the function $\varphi$. However, this gradient is not always known and can be expensive to evaluate in high dimensions; in some cases, the function $\varphi$ is even not differentiable, as will be the case in our numerical example in @sec-sub:portfolio. </span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>In contrast, our method makes no assumption on the form or smoothness of $\phi$: it does not need to assume that it is of the form $\mathbb{I}_{<span class="sc">\{</span>\varphi \geq 0<span class="sc">\}</span>}$, or to assume that $\nabla \varphi$ is tractable. For completeness, whenever  the algorithm of <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span> was applicable and computing the gradient of $\varphi$ did not require any additional simulation budget, we have run it on the test cases considered here and found that it outperformed our algorithm. In more realistic settings, computing $\nabla \varphi$ would likely increase the simulation budget, and it would be interesting to compare the two algorithms in more details to understand when this extra computation cost is worthwhile. We reserve such a question for future research and will not consider the algorithm of <span class="co">[</span><span class="ot">@UribeEtAl_CrossentropybasedImportanceSampling_2020</span><span class="co">]</span> further, as our aim in this paper is to establish benchmark results for a general algorithm which works for any function $\phi$. </span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a><span class="fu">## Definition of the function $\ell$</span></span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a>The statement of our result involves the following function $\ell$, which is represented in @fig-l:</span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a>    $$</span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a>    \ell: x \in (0,\infty) \mapsto -\log(x) + x - 1.</span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>    $$ {#eq-l}</span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a>In the following, $(\lambda, \mathbf{d}) \in \mathbb{R} \times \mathbb{R}^n$ is an eigenpair of a matrix $A$ if $A\mathbf{d} = \lambda \mathbf{d}$ and $\lVert \mathbf{d} \rVert = 1$. A diagonalizable matrix has $n$ distinct eigenpairs, say $((\lambda_i, \mathbf{d}_i), i = 1, \ldots, n)$, and we say that these eigenpairs are ranked in decreasing $\ell$-order if $\ell(\lambda_1) \geq \cdots \geq \ell(\lambda_n)$.</span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a>In the rest of the article, we denote as $(\lambda^*_i, \mathbf{d}^*_i)$ the eigenpairs of $\mathbf{\Sigma}^*$ ranked in decreasing $\ell$-order and as $({\widehat{\lambda}}^*_i, \widehat{\mathbf{d}}^*_i)$ the eigenpairs of $\widehat{\mathbf{\Sigma}}^*$ ranked in decreasing $\ell$-order.</span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-l</span></span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: Plot of the function $\ell$ given by @eq-l.</span></span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 1. Plot of the function "l"</span></span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a><span class="co">#######################################################################</span></span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.stats</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a><span class="co">### the following library is available on the following website : </span></span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a><span class="co">### "Papaioannou, I., Geyer, S., and Straub, D. (2019b). </span></span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a><span class="co">### Software tools for reliability analysis :</span></span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a><span class="co">### Cross entropy method and improved cross entropy method. Retrieved from </span></span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a><span class="co">### https://www.cee.ed.tum.de/en/era/software/reliability/"</span></span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> CEIS_vMFNM <span class="im">import</span> <span class="op">*</span>      </span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> display, Math, Latex</span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tabulate <span class="im">import</span> tabulate</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">10</span>)</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(np.finfo(<span class="bu">float</span>).eps,<span class="fl">4.0</span>,<span class="dv">100</span>)</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="op">-</span>np.log(x) <span class="op">+</span> x <span class="op">-</span><span class="dv">1</span></span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a><span class="co"># plot</span></span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>ax.plot(x, y, linewidth<span class="op">=</span><span class="fl">2.0</span>)</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">4</span>), xticks<span class="op">=</span>[<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>],</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>       ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), yticks<span class="op">=</span>[<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dv">1</span>,<span class="fl">1.5</span>])</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$x$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(x)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a><span class="fu">## Main result of the paper {#sec-main-result-positioning} </span></span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>The main result of the present paper is to compute the exact value for $\mathbf{\mathbf{\Sigma}}^*_k$ in @eq-argminDkl-k, which therefore paves the way for efficient high-dimensional estimation schemes. </span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>::: {#thm-thm1}</span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>Let $(\lambda^*_i, \mathbf{d}^*_i)$ be the eigenpairs of $\mathbf{\Sigma}^*$ ranked in decreasing $\ell$-order. Then for $1 \leq k \leq n$, the solution $(\mathbf{m}^*_k, \mathbf{\Sigma}^*_k)$ to @eq-argminDkl-k is given by</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>\mathbf{m}^*_k = \mathbf{m}^* \ \text{ and } \ \mathbf{\Sigma}^*_k = I_n + \sum_{i=1}^k \left( \lambda^*_i - 1 \right) \mathbf{d}^*_i (\mathbf{d}^*_i)^\top. </span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a>$$ {#eq-Sigma-k}</span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>The proof of @thm-thm1 is detailed in <span class="co">[</span><span class="ot">Appendix A</span><span class="co">](#sec-proof)</span>. For $k = 1$ for instance, the matrix $\mathbf{\Sigma}^*_1 = I_n + (\lambda_1^*-1) \mathbf{d}_1^* (\mathbf{d}_1^*)^\top$ with $(\lambda_1^*, \mathbf{d}_1^*)$ the eigenpair of $\mathbf{\Sigma}^*$ such as $\lambda_1^*$ is either the largest or the smallest eigenvalue of $\mathbf{\Sigma}^*$, depending on which one maximizes $\ell$.</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>This theoretical result therefore suggests to reduce dimension by computing the covariance matrix $\widehat{\mathbf{\Sigma}}^*$ and its eigenpairs, rank them in decreasing $\ell$-order and then use the $k$ first eigenpairs $(({\widehat{\lambda}}^*_i, {\widehat{\mathbf{d}}}^*_i), i = 1, \ldots, k)$ to build the covariance matrix $\widehat{\mathbf{\Sigma}}^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n$ and the corresponding auxiliary density. This scheme is summarized in Algorithm 1. The effective dimension $k$ is obtained by Algorithm 2, see @sec-choicek below. The proof of the theorem is shown in <span class="co">[</span><span class="ot">Appendix A</span><span class="co">](#sec-proof)</span>.</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a><span class="in">```{.pseudocode}</span></span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{Algorithm suggested by Theorem 1.}</span></span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Data}: Sample sizes $N$ and $M$</span></span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Result}: Estimation $\widehat{\mathcal{E}_N}$ of integral $\mathcal{E}$</span></span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Generate a sample $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$  on $\mathbb{R}^n$ independently according to $g^*$</span></span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Estimate $\widehat{\mathbf{m}}^*$ and $\widehat{\mathbf{\Sigma}}^*$ defined in Equation 8 and Equation 9 with this sample</span></span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Compute the eigenpairs $(\widehat{\lambda}^*_i, \widehat{\mathbf{d}}^*_i)$ of $\widehat{\mathbf{\Sigma}}^*$ ranked in decreasing $\ell$-order</span></span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Compute the matrix $\widehat{\mathbf{\Sigma}}^*_k = \sum_{i=1}^k ({\widehat{\lambda}}^*_i-1) {\widehat{\mathbf{d}}}^*_i ({{\widehat{\mathbf{d}}}^*}_i)^\top + I_n$ with $k$ obtained by applying Algorithm 2 with input $({\widehat{\lambda}}^*_1, \ldots, {\widehat{\lambda}}^*_n)$</span></span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Generate a new sample $\mathbf{X}_1,\ldots,\mathbf{X}_N$ independently from $g' = g_{\widehat{\mathbf{m}}^*,\widehat{\mathbf{\Sigma}}^*_k}$</span></span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Return $\displaystyle \widehat{\mathcal{E}_N}=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}$</span></span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a>::: {.remark}</span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a>Since the function $\ell$ is minimized at 1, eigenpairs with $\lambda^*_i =1$ are selected in the sum of @eq-Sigma-k once all other eigenpairs have been picked as the eigenpairs are $\ell$-ordered: in other words, if $\lambda^*_i = 1$ then $\lambda^*_j = 1$ for all $j \geq i$. Note also that the minimizer $1$ plays a special role as we are interested in covariance matrices of $\mathcal{L}_{n,k}$ which, once diagonalized, have mostly ones in the main diagonal (except for k values associated with the $\alpha_i$). As $k$ will be small (See @sec-choicek), typically $k = 1$ or $2$, this amounts to finding covariance matrices that are perturbations of the identity (this is relevant as we assume $f$ is standard Gaussian). Therefore, when approximating $\mathbf{\Sigma}^*$ by such matrices, we should first consider eigenvalues as different as possible from $1$ (with the discrepancy from 1 being measured by $\ell$).</span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>In the first step of Algorithm 1, we assume $g^*$ can be sampled independently. This is a reasonable assumption as classical techniques such as importance sampling with self-normalized weights or Markov Chain Monte Carlo (MCMC) can be applied in this case (see for instance [@ChanKroese_ImprovedCrossentropyMethod_2012], [@GraceEtAl_AutomatedStateDependentImportance_2014]). In this paper, we choose to apply a basic rejection method that yields perfect independent samples from $g^*$, possibly at the price of a high computational cost. As the primary goal of this paper is to understand whether the $\mathbf{d}^*_i$'s are indeed good projection directions, this cost will not be taken into account. Possible improvements to relax this assumption are discussed in the conclusion of the paper and in <span class="co">[</span><span class="ot">Appendix C</span><span class="co">](#sec-MCMC)</span>.</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choice of the number of dimensions $k$ {#sec-choicek} </span></span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a>The choice of the effective dimension $k$, i.e., the number of projection directions considered, is important. If it is close to $n$, then the matrix $\widehat{\mathbf{\Sigma}}^*_k$ will be close to $\widehat{\mathbf{\Sigma}}^*$ which is the situation we want to avoid in the first place. On the other hand, setting $k=1$ in all cases may be too simple and lead to suboptimal results. In practice, however this is often a good choice. In order to adapt $k$ dynamically, we consider a simple method based on the value of the KL divergence. Given the eigenvalues $\lambda_1, \ldots, \lambda_n$ ranked in decreasing $\ell$-order, we look for the maximal gap between two consecutive eigenvalues of the sequence $(\ell(\lambda_1), \ldots, \ell(\lambda_n))$. This allows to choose $k$ such that $\sum_{i=1}^k \ell(\lambda_i)$ is close to $\sum_{i=1}^n \ell(\lambda_i)$ which is equal, up to an additive constant, to the minimal KL divergence (shown in @lem-D). The precise method is described in Algorithm 2. </span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a><span class="in">```{.pseudocode}</span></span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithm}</span></span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a><span class="in">\caption{Choice of the number of dimensions}</span></span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a><span class="in">\begin{algorithmic}</span></span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Data}: Sequence of positive numbers $\lambda_1, \ldots, \lambda_n$ in decreasing $\ell$-order</span></span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a><span class="in">\State \textbf{Result}: Number of selected dimensions $k$</span></span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Compute the increments $\delta_i = \ell(\lambda_{i+1}) - \ell(\lambda_i)$ for $i=1\ldots n-1$</span></span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a><span class="in">\State - Return $k=\arg\max \delta_i$, the index of the maximum of the differences.</span></span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithmic}</span></span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a><span class="in">\end{algorithm}</span></span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a><span class="fu">## Theoretical result concerning the projection on $\mathbf{m}^*$ {#sec-mm} </span></span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a>In <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span>, the authors propose to project on the mean $\mathbf{m}^*$ of the optimal auxiliary density $g^*$. Numerically, this algorithm is shown to perform well, but only a very heuristic explanation based on the light tail of the Gaussian distribution is provided to motivate this choice. It turns out that the techniques used in the proof of @thm-thm1 can shed light on why projecting on $\mathbf{m}^*$ may indeed be a good idea. Let us first state our theoretical result, and then explain why it justifies the idea of projecting on $\mathbf{m}^*$.</span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a>::: {#thm-thm2}</span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a>Consider $\mathbf{\Sigma} \in \mathcal{L}_{n,1}$ of the form $\mathbf{\Sigma} = I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top$ with $\alpha &gt; 0$ and $\lVert \mathbf{d} \rVert = 1$. Then the minimizer in $(\alpha, \mathbf{d})$ of the KL divergence between $f$ and $g_{\mathbf{m}^*, \mathbf{\Sigma}}$ is $(1+\lVert \mathbf{m}^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert)$:</span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a>        $$\left( 1+\lVert \mathbf{m}^*\rVert^2, \mathbf{m}^* / \lVert \mathbf{m}^* \rVert \right) = \arg \min_{\alpha, \mathbf{d}} \left\{ D(f, g_{\mathbf{m}^*, I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top}): \alpha &gt; 0, \ \lVert \mathbf{d} \rVert = 1 \right<span class="sc">\}</span>. $$</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a>The proof of @thm-thm2 is detailed in <span class="co">[</span><span class="ot">Appendix A</span><span class="co">](#sec-proof)</span>. In other words, $\mathbf{m}^*$ appears as an optimal projection direction when one seeks to minimize the KL divergence between $f$ and the Gaussian density with mean $\mathbf{m}^*$ and covariance of the form $I_n + (\alpha - 1) \mathbf{d} \mathbf{d}^\top$. Let us now explain why this minimization problem is indeed relevant, and why choosing an auxiliary density which minimizes this KL divergence may indeed lead to an accurate estimation. The justification deeply relies on the recent results by <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span>.</span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a>As mentioned above, in a reliability context where one seeks to estimate a small probability $p = \mathbb{P}(\mathbf{X} \in A),$ Theorem $1.3$ in <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span> shows that $D(g^*, g)$ governs the sample size required for an accurate estimation of $p$: more precisely, the estimation is accurate if the sample size is larger than $e^{D(g^*, g)}$, and inaccurate otherwise. This motivates the rationale for minimizing the KL divergence with $g^*$.</span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a>However, in high dimensions, importance sampling is known to fail because of the weight degeneracy problem whereby $\max_i L_i / \sum_i L_i \approx 1$, with the $L_i$'s the unnormalized importance weights, or likelihood ratios: $L_i = f(\mathbf{X}_i) / g(\mathbf{X}_i)$ with the $\mathbf{X}_i$'s i.i.d. drawn according to $g$. Theorem $2.3$ in <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span> shows that the weight degeneracy problem is avoided if the empirical mean of the likelihood ratios is close to $1$, and for this, Theorem $1.1$ in <span class="co">[</span><span class="ot">@Chatterjee18:0</span><span class="co">]</span> shows that the sample size should be larger than $e^{D(f, g)}$. In other words, these results suggest that the KL divergence with $g^*$ governs the sample size for an accurate estimation of $p$, while the KL divergence with $f$ governs the weight degeneracy problem.</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a>In light of these results, it becomes natural to consider the KL divergence with $f$ and not only $g^*$ [@OwenZhou_SafeEffectiveImportance_2000]. Of course, minimizing $D(f, g_{\mathbf{m}, \mathbf{\Sigma}})$ without constraints on $\mathbf{m}$ and $\mathbf{\Sigma}$ is trivial since $g_{\mathbf{m}, \mathbf{\Sigma}} = f$ for $\mathbf{m} = 0$ and $\mathbf{\Sigma} = I_n$. However, these choices are the ones we want to avoid in the first place, and so it makes sense to impose some constraints on $\mathbf{m}$ and $\mathbf{\Sigma}$. If one keeps in mind the other objective of getting close to $g^*$, then the choice $\mathbf{m} = \mathbf{m}^*$ becomes very natural, and we are led to considering the optimization problem of @thm-thm2 (when $\mathbf{\Sigma} \in \mathcal{L}_{n,1}$ is a rank-1 perturbation of the identity).</span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a><span class="fu"># Computational framework {#sec-num-results-framework} </span></span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a><span class="fu">## Numerical procedure for IS estimate comparison {#sec-def_proc} </span></span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a>The objective of the numerical simulations is to evaluate the impact of the choice of the covariance matrix on the estimation accuracy of a high dimensional integral $\mathcal{E}$. We thus want to compare the IS estimation results for different auxiliary densities and more particularly for different choices of the auxiliary covariance matrix when the IS auxiliary density is Gaussian. The details of the considered covariance matrices is given in @sec-def_cov. To extend this comparison, we also compute the results when the IS auxiliary density is chosen with the von Mises--Fisher--Nakagami (vMFN) model recently proposed in <span class="co">[</span><span class="ot">@PapaioannouEtAl_ImprovedCrossEntropybased_2019</span><span class="co">]</span> for high dimensional probability estimation (See <span class="co">[</span><span class="ot">Appendix B</span><span class="co">](#sec-naka)</span>). </span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a>In @sec-test-cases we test these different models of auxiliary densities on five test cases, where $f$ is a standard Gaussian density. This choice is not a theoretical limitation as we can in principle always come back to this case by transforming the vector $\mathbf{X}$ with isoprobabilistic transformations (see for instance <span class="co">[</span><span class="ot">@HohenbichlerRackwitz_NonNormalDependentVectors_1981</span><span class="co">]</span>, <span class="co">[</span><span class="ot">@LiuDerKiureghian_MultivariateDistributionModels_1986</span><span class="co">]</span>).</span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a>The precise numerical framework that we will consider to assess the efficiency of the different auxiliary models is as follows. We assume first that $M$ i.i.d.\ random samples $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$ distributed from $g^*$ are available from rejection sampling (unless in <span class="co">[</span><span class="ot">Appendix C</span><span class="co">](#sec-MCMC)</span> where we consider MCMC). From these samples, the parameters of the Gaussian and of the vMFN auxiliary density are computed to get an auxiliary density $g'$. Finally, $N$ samples are generated from $g'$ to provide an estimation of $\mathcal{E}$ with IS. This procedure is summarized by the following stages: </span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Generate a sample $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$ independently according to $g^*$;</span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>From $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$, compute the parameters of the auxiliary parametric density $g'$;</span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Generate a new sample $\mathbf{X}_1,\ldots,\mathbf{X}_N$ independently from $g'$;</span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Estimate $\mathcal{E}$ with $\widehat{\mathcal{E}_N}=\frac{1}{N}\underset{i=1}{\overset{N}{\sum}} \phi(\mathbf{X}_i)\frac{f(\mathbf{X}_i)}{g'(\mathbf{X}_i)}$.</span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>The number of samples $M$ and $N$ are respectively set to $M=500$ and $N=2000$. The computational cost to generate $M=500$ samples distributed from $g^*$ with rejection sampling is often unaffordable in practice; if $\mathcal{E}$ is a probability of order $10^{-p}$, then approximately $500\times10^p$ calls to $\phi$ are necessary for the generation of $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$. Finally, whatever the auxiliary parametric density $g'$ computed from $\mathbf{X}^*_1,\ldots,\mathbf{X}^*_M$, the number of calls to $\phi$ for the estimation step stays constant and equal to $N$. The number of calls to $\phi$ for the whole procedure on a $10^{-p}$ probability estimation is about $500\times10^p+N$. A more realistic situation is considered in [Appendix C](#sec-MCMC) where MCMC is applied to generate samples from $g^*$. The resulting samples are dependent but the computational cost is significanlty reduced. The number of calls to $\phi$ with MCMC is then equal to $M$ which leads to a total computational cost of $M+N$ for the whole procedure.</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>This procedure is then repeated $500$ times to provide a mean estimation $\widehat{\mathcal{E}}$ of $\mathcal{E}$. In the result tables, for each auxiliary density $g'$ we report the corresponding value for the relative error $\widehat{\mathcal{E}}/ \mathcal{E}-1$ and the coefficient of variation of the $500$ iterations (the empirical standard deviation divided by $\mathcal{E}$). As was established in the proof of @thm-thm1, the KL divergence is, up to an additive constant, equal to $D'(\mathbf{\Sigma}) = \log \lvert \mathbf{\Sigma} \rvert + \textrm{tr}(\mathbf{\Sigma}^* \mathbf{\Sigma}^{-1})$ which we will refer to as partial KL divergence. In the result tables, we also report thus the mean value of $D'(\mathbf{\Sigma})$ to analyse the relevance of the auxiliary density $g_{\widehat{\mathbf{m}}^*, \mathbf{\Sigma}}$ for six choices of covariance matrix $\mathbf{\Sigma}$. The next sections specify the different parameters of $g'$ for the Gaussian model and for the vMFN model we have considered in the simulations. </span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a><span class="fu">## Choice of the auxiliary density $g'$ for the Gaussian model  {#sec-def_cov} </span></span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>The goal is to get benchmark results to assess whether one can improve estimations of Gaussian IS auxiliary density by projecting the covariance matrix $\mathbf{\Sigma}^*$ in the proposed directions $\mathbf{d}^*_i$. The algorithm that we study here (Algorithms 1+2) aims more precisely at understanding whether:</span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>projecting can improve the situation with respect to the empirical covariance matrix;</span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the $\mathbf{d}^*_i$'s are good candidates, in particular compared to the choice $\mathbf{m}^*$ suggested in <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span>;</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>what is the impact in making errors in estimating the eigenpairs $(\lambda^*_i, \mathbf{d}^*_i)$.</span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a>Let us define the estimate  $\widehat{\mathbf{m}}^*$ of $\mathbf{m}^*$  from the $M$ i.i.d. random samples $\mathbf{X}_1^*,\ldots,\mathbf{X}_M^*$ distributed from $g^*$ with</span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a>    \widehat{\mathbf{m}}^* = \frac{1}{M}\sum_{i=1}^M \mathbf{X}_i^*.</span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a>$$ {#eq-hatm}</span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>In our numerical test cases, we will compare six different choices of Gaussian auxiliary distributions $g'$ with mean $\widehat{\mathbf{m}}^*$ and the following covariance matrices summarized in @tbl-sigma:</span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>$\mathbf{\Sigma}^*$: the optimal covariance matrix given by @eq-mstar;</span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>$\widehat{\mathbf{\Sigma}}^*$: the empirical estimation of $\mathbf{\Sigma}^*$ given by</span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a>    \widehat{\mathbf{\Sigma}}^* = \frac{1}{M}\sum_{i=1}^M (\mathbf{X}_i^*-\widehat{\mathbf{m}}^*)(\mathbf{X}_i^*-\widehat{\mathbf{m}}^*)^\top.</span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a>$$ {#eq-hatSigma}</span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a>The four other covariance matrices considered in the numerical simulations are of the form </span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a>     $\sum_{i=1}^k (v_i-1) \mathbf{d}_i \mathbf{d}^\top_i + I_n$ where $v_i$ is the variance of $\widehat{\mathbf{\Sigma}}^*$ in the direction $\mathbf{d}_i$, $v_i = \mathbf{d}_i^\top \widehat{\mathbf{\Sigma}}^* \mathbf{d}_i$. The considered choice of $k$ and $\mathbf{d}_i$ gives the following covariance matrices:   </span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ is obtained by choosing $\mathbf{d}_i = \mathbf{d}^*_i$ of @thm-thm1, which is supposed to be perfectly known from $\mathbf{\Sigma}^*$ and $k$ is computed with Algorithm 2;</span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$ is obtained by choosing $\mathbf{d}_i = {\widehat{\mathbf{d}}}^*_i$ the $i$-th eigenvector of $\widehat{\mathbf{\Sigma}}^*$ (in $\ell$-order), which is an estimation of $\mathbf{d}^*_i$, and $k$ is computed with Algorithm 2;</span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ is obtained by choosing $k = 1$ and $\mathbf{d}_1 = \mathbf{m}^* / \lVert \mathbf{m}^* \rVert$;  </span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ is obtained by choosing $k = 1$ and $\mathbf{d}_1 = {\widehat{\mathbf{m}}}^* / \lVert {\widehat{\mathbf{m}}}^* \rVert$, where $\widehat{\mathbf{m}}^*$ given by @eq-hatm.  </span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a>The matrices ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ use the estimation $\widehat{\mathbf{\Sigma}}^*$ with the optimal directions $\mathbf{d}^*_i$ or $\mathbf{m}^*$, while the matrices ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ involve an estimation of these directions from $\widehat{\mathbf{\Sigma}}^*$. By definition, $\mathbf{\Sigma}^*$ will give optimal results, while results for $\widehat{\mathbf{\Sigma}}^*$ will deteriorate as the dimension increases, which is the well-known behavior which we try to improve. Moreover, $\mathbf{\Sigma}^*$ and the projection directions $\mathbf{d}^*_i$ or $\mathbf{m}^*$, are of course unknown in practice. For simulation comparison purpose, they could be determined analytically in simple test cases and otherwise we obtained them by a brute force Monte Carlo scheme with a very high simulation budget. Finally, we emphasize that Algorithm 1 corresponds to estimating and projecting on the $\mathbf{d}^*_i$'s, and so the matrix $\widehat{\mathbf{\Sigma}}^*_k$ of Algorithm 1 is equal to the matrix ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$. </span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>|   |$\mathbf{\Sigma}^*$|$\widehat{\mathbf{\Sigma}}^*$|${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$|${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$|${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$|${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$|</span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a>|-------|---|---|---|---|---|---|---|</span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a>|Initial covariance matrix|$\mathbf{\Sigma}^*$|$\widehat{\mathbf{\Sigma}}^*$|$\widehat{\mathbf{\Sigma}}^*$|$\widehat{\mathbf{\Sigma}}^*$|$\widehat{\mathbf{\Sigma}}^*$|$\widehat{\mathbf{\Sigma}}^*$|</span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a>|Projection directions (exact or estimated)|-|-|Exact|Exact|Estimated|Estimated|</span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a>|Choice for the projection direction|None|None|Opt|Mean|Opt|Mean|</span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a>:  Presentation of the six covariance matrices considered in the numerical examples. {#tbl-sigma}</span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a><span class="fu">#  Numerical results on five test cases   {#sec-test-cases} </span></span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a> The proposed numerical framework is applied on three examples that are often considered to assess the performance of importance sampling algorithms and also two test cases  from the area of financial mathematics. </span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a><span class="fu">##  Test case 1: one-dimensional optimal projection   {#sec-sub:sum} </span></span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a>We consider a test case where all computations can be made exactly. This is a classical example of rare event probability estimation, often used to test the robustness of a method in high dimensions. It is given by $\phi(\mathbf{x})=\mathbb{I}_{<span class="sc">\{</span>\varphi(\mathbf{x})\geq 0<span class="sc">\}</span>}$ with $\varphi$ the following affine function:</span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a>    \varphi: \mathbf{x}=(x_1,\ldots,x_n)\in\mathbb{R}^n \mapsto\underset{j=1}{\overset{n}{\sum}} x_j-3\sqrt{n}.</span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a>$${#eq-sum}</span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a>The quantity of interest $\mathcal{E}$ is defined as $\mathcal{E}=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)\simeq 1.35\cdot 10^{-3}$ for all $n$ where the density $f$ is the standard $n$-dimensional Gaussian distribution. Here, the zero-variance density is $g^*(\mathbf{x})=\dfrac{f(\mathbf{x})\mathbb{I}_{<span class="sc">\{</span>\varphi(\mathbf{x})\geq 0<span class="sc">\}</span>}}{\mathcal{E}}$, and the optimal parameters $\mathbf{m}^*$ and $\mathbf{\Sigma}^*$ in @eq-mstar can be computed exactly, namely $\mathbf{m}^* = \alpha \textbf{1}$ with $\alpha = e^{-9/2}/(\mathcal{E}(2\pi)^{1/2})$ and $\textbf{1} = \frac{1}{\sqrt n} (1,\ldots,1) \in \mathbb{R}^n$ the normalized constant vector, and $\mathbf{\Sigma}^* =(v-1) \mathbf{1} \mathbf{1}^\top + I_n$ with $v=3\alpha-\alpha^2+1$.</span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a><span class="fu">###  Evolution of the partial KL divergence and spectrum</span></span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a>@fig-eigsum-1 represents the evolution as the dimension varies between $5$ and $100$ of the partial KL divergence $D'$ for three different choices of covariance matrix: the optimal matrix $\mathbf{\Sigma}^*$, its empirical estimation $\widehat{\mathbf{\Sigma}}^*$ and the estimation $\widehat{\mathbf{\Sigma}}^*_k$ of the optimal lower-dimensional covariance matrix. We can notice that the partial KL divergence for $\widehat{\mathbf{\Sigma}}^*$ grows much faster than the other two, and that the partial KL divergence for $\widehat{\mathbf{\Sigma}}^*_k$ remains very close to the optimal value $D'(\mathbf{\Sigma}^*)$. As the KL divergence is a proxy for the efficiency of the auxiliary density (it is for instance closely related to the number of samples required for a given precision [@Chatterjee18:0]), this suggests that using $\widehat{\mathbf{\Sigma}}^*_k$ will provide results close to optimal.</span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a>We now check this claim. As $\mathbf{\Sigma}^* = (v-1) \textbf{1} \textbf{1}^\top + I_n$, its eigenpairs are $(v, \textbf{1})$ and $(1,\mathbf{d}_i)$ where the $\mathbf{d}_i$'s form an orthonormal basis of the space orthogonal to the space spanned by $\textbf{1}$. In particular, $(v, \textbf{1})$ is the largest (in $\ell$-order) eigenpair of $\mathbf{\Sigma}^*$ and $\mathbf{\Sigma}^*_k = \mathbf{\Sigma}^*$ for any $k \geq 1$.</span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a>In practice, we do not use this theoretical knowledge and $\mathbf{\Sigma}^*$, $\mathbf{\Sigma}^*_k$ and the eigenpairs are estimated. The six covariance matrices introduced in @sec-def_cov and in which we are interested are as follows:</span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\mathbf{\Sigma}^* = (v-1) \textbf{1} \textbf{1}^\top + I_n$;</span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>$\widehat{\mathbf{\Sigma}}^*$ given by @eq-hatSigma;</span>
<span id="cb4-330"><a href="#cb4-330" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ are equal and given by $(\widehat \lambda-1) \textbf{1} \textbf{1}^\top + I_n$ with $\widehat{\lambda} = \textbf{1}^\top \widehat{\mathbf{\Sigma}}^* \textbf{1}$. This amounts to assuming that the projection direction $\textbf{1}$ is perfectly known, whereas the variance in this direction is estimated;</span>
<span id="cb4-331"><a href="#cb4-331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}} = (\widehat{\lambda} - 1) \widehat{\mathbf{d}} {\widehat{\mathbf{d}}}^\top + I_n$ with $(\widehat{\lambda}, \widehat{\mathbf{d}})$ the smallest eigenpair of $\widehat{\mathbf{\Sigma}}^*$. The difference with the previous case is that we do not assume anymore that the optimal projection direction $\textbf{1}$ is known, and so it needs to be estimated;</span>
<span id="cb4-332"><a href="#cb4-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}} = (\widehat{\lambda} - 1) \frac{\widehat{\mathbf{m}}^* {(\widehat{\mathbf{m}}^*)}^\top}{\lVert \widehat{\mathbf{m}}^* \rVert^2} + I_n$ with $\widehat{\mathbf{m}}^*$ given by @eq-hatm and $\widehat{\lambda} = \frac{{(\widehat{\mathbf{m}}^*)}^\top \widehat{\mathbf{\Sigma}}^* \widehat{\mathbf{m}}^*}{\lVert \widehat{\mathbf{m}}^* \rVert^2}$. Here we assume that $\mathbf{m}^*$ is a good projection direction, but is unknown and therefore needs to be estimated.</span>
<span id="cb4-333"><a href="#cb4-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-334"><a href="#cb4-334" aria-hidden="true" tabindex="-1"></a>Note that in the particularly simple case considered here, both $\widehat{\mathbf{m}}^* / \lVert \widehat{\mathbf{m}}^* \rVert$ and $\widehat{\mathbf{d}}$ are estimators of $\textbf{1}$ but they are obtained by different methods. In the next example we will consider a case where $\mathbf{m}^*$ is not an optimal projection direction as given by @thm-thm1.</span>
<span id="cb4-335"><a href="#cb4-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-336"><a href="#cb4-336" aria-hidden="true" tabindex="-1"></a>@fig-eigsum-2 represents the images by $\ell$ of the eigenvalues of $\mathbf{\Sigma}^*$ and $\widehat{\mathbf{\Sigma}}^*$. This picture carries a very important insight. We notice that the estimation of most eigenvalues is poor: indeed, all the blue crosses except the leftmost one are meant to be estimator of $1$, whereas we see that they are more or less uniformly spread around $1$. This means that the variance terms in the corresponding directions are poorly estimated, which could be the explanation on why the use of $\widehat{\mathbf{\Sigma}}^*$ gives an inaccurate estimation. But what we remark also is that the function $\ell$ is quite flat around one: as a consequence, although the eigenvalues offer significant variability, this variability is smoothed by the action of $\ell$. Indeed, the images of the eigenvalues by $\ell$ take values between $0$ and $0.8$ and have smaller variability. Moreover, $\ell(x)$ increases sharply as $x$ approaches $0$ and thus efficiently distinguishes between the two leftmost estimated eigenvalues and is able to separate them.</span>
<span id="cb4-337"><a href="#cb4-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-340"><a href="#cb4-340" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-341"><a href="#cb4-341" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-eigsum</span></span>
<span id="cb4-342"><a href="#cb4-342" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: 'Partial KL divergence and spectrum for the function $\phi = \mathbb{I}_{\varphi \geq 0}$ with $\varphi$ the linear function given by @eq-sum.'</span></span>
<span id="cb4-343"><a href="#cb4-343" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-subcap:</span></span>
<span id="cb4-344"><a href="#cb4-344" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Evolution of the partial KL divergence as the dimension increases, with the optimal covariance matrix $\mathbf{\Sigma}^*$ (red squares), the sample covariance $\widehat{\mathbf{\Sigma}}^*$ (blue circles), and the projected covariance $\widehat{\mathbf{\Sigma}}^*_k$ (black dots).'</span></span>
<span id="cb4-345"><a href="#cb4-345" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - 'Computation of $\ell(\lambda_i)$ for the eigenvalues of $\mathbf{\Sigma}^*$ (red squares) and $\widehat{\mathbf{\Sigma}}^*$ (blue crosses) in dimension $n = 100$.'</span></span>
<span id="cb4-346"><a href="#cb4-346" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout:</span></span>
<span id="cb4-347"><a href="#cb4-347" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb4-348"><a href="#cb4-348" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb4-349"><a href="#cb4-349" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb4-350"><a href="#cb4-350" aria-hidden="true" tabindex="-1"></a><span class="co">#|   - - 45</span></span>
<span id="cb4-351"><a href="#cb4-351" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - -10</span></span>
<span id="cb4-352"><a href="#cb4-352" aria-hidden="true" tabindex="-1"></a><span class="co">#|     - 45</span></span>
<span id="cb4-353"><a href="#cb4-353" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb4-354"><a href="#cb4-354" aria-hidden="true" tabindex="-1"></a><span class="co"># Figure 2. Evolution of the partial KL divergence and spectrum of the</span></span>
<span id="cb4-355"><a href="#cb4-355" aria-hidden="true" tabindex="-1"></a><span class="co"># eigenvalues for the test case 1</span></span>
<span id="cb4-356"><a href="#cb4-356" aria-hidden="true" tabindex="-1"></a><span class="co">#############################################################################</span></span>
<span id="cb4-357"><a href="#cb4-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-358"><a href="#cb4-358" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Somme(x):</span>
<span id="cb4-359"><a href="#cb4-359" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span>np.shape(x)[<span class="dv">1</span>]</span>
<span id="cb4-360"><a href="#cb4-360" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span>(np.<span class="bu">sum</span>(x,axis<span class="op">=</span><span class="dv">1</span>)<span class="op">-</span><span class="dv">3</span><span class="op">*</span>np.sqrt(n))</span>
<span id="cb4-361"><a href="#cb4-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-362"><a href="#cb4-362" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb4-363"><a href="#cb4-363" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Somme</span>
<span id="cb4-364"><a href="#cb4-364" aria-hidden="true" tabindex="-1"></a>E<span class="op">=</span>sp.stats.norm.cdf(<span class="op">-</span><span class="dv">3</span>)   <span class="co"># exact value of the integral</span></span>
<span id="cb4-365"><a href="#cb4-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-366"><a href="#cb4-366" aria-hidden="true" tabindex="-1"></a>DKL<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb4-367"><a href="#cb4-367" aria-hidden="true" tabindex="-1"></a>DKLp<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb4-368"><a href="#cb4-368" aria-hidden="true" tabindex="-1"></a>DKLm<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb4-369"><a href="#cb4-369" aria-hidden="true" tabindex="-1"></a>DKLstar<span class="op">=</span>np.zeros(<span class="dv">20</span>)</span>
<span id="cb4-370"><a href="#cb4-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-371"><a href="#cb4-371" aria-hidden="true" tabindex="-1"></a>M<span class="op">=</span><span class="dv">300</span></span>
<span id="cb4-372"><a href="#cb4-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-373"><a href="#cb4-373" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">5</span>,n<span class="op">+</span><span class="dv">1</span>,<span class="dv">5</span>):</span>
<span id="cb4-374"><a href="#cb4-374" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Mstar</span></span>
<span id="cb4-375"><a href="#cb4-375" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span>np.exp(<span class="op">-</span><span class="dv">3</span><span class="op">**</span><span class="dv">2</span><span class="op">/</span><span class="dv">2</span>)<span class="op">/</span>(E<span class="op">*</span>np.sqrt(<span class="dv">2</span><span class="op">*</span>np.pi))</span>
<span id="cb4-376"><a href="#cb4-376" aria-hidden="true" tabindex="-1"></a>    Mstar<span class="op">=</span>alpha<span class="op">*</span>np.ones(d)<span class="op">/</span>np.sqrt(d)</span>
<span id="cb4-377"><a href="#cb4-377" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sigmastar</span></span>
<span id="cb4-378"><a href="#cb4-378" aria-hidden="true" tabindex="-1"></a>    vstar<span class="op">=</span><span class="dv">3</span><span class="op">*</span>alpha<span class="op">-</span>alpha<span class="op">**</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span></span>
<span id="cb4-379"><a href="#cb4-379" aria-hidden="true" tabindex="-1"></a>    Sigstar<span class="op">=</span> (vstar<span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>np.ones((d,d))<span class="op">/</span>d<span class="op">+</span>np.eye(d)</span>
<span id="cb4-380"><a href="#cb4-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-381"><a href="#cb4-381" aria-hidden="true" tabindex="-1"></a>    <span class="co">## g*-sample</span></span>
<span id="cb4-382"><a href="#cb4-382" aria-hidden="true" tabindex="-1"></a>    VA0<span class="op">=</span>sp.stats.multivariate_normal(mean<span class="op">=</span>np.zeros(d),cov<span class="op">=</span>np.eye(d))</span>
<span id="cb4-383"><a href="#cb4-383" aria-hidden="true" tabindex="-1"></a>    X0<span class="op">=</span>VA0.rvs(size<span class="op">=</span>M<span class="op">*</span><span class="dv">1000</span>)</span>
<span id="cb4-384"><a href="#cb4-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-385"><a href="#cb4-385" aria-hidden="true" tabindex="-1"></a>    ind<span class="op">=</span>(phi(X0)<span class="op">&gt;</span><span class="dv">0</span>)</span>
<span id="cb4-386"><a href="#cb4-386" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X0[ind,:]</span>
<span id="cb4-387"><a href="#cb4-387" aria-hidden="true" tabindex="-1"></a>    X<span class="op">=</span>X[:M,:]            <span class="co"># g*-sample of size M</span></span>
<span id="cb4-388"><a href="#cb4-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-389"><a href="#cb4-389" aria-hidden="true" tabindex="-1"></a>    <span class="co">## estimated mean and covariance</span></span>
<span id="cb4-390"><a href="#cb4-390" aria-hidden="true" tabindex="-1"></a>    mm<span class="op">=</span>np.mean(X,axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-391"><a href="#cb4-391" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-392"><a href="#cb4-392" aria-hidden="true" tabindex="-1"></a>    Xc<span class="op">=</span>(X<span class="op">-</span>mm).T</span>
<span id="cb4-393"><a href="#cb4-393" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span>Xc <span class="op">@</span> Xc.T<span class="op">/</span>np.shape(Xc)[<span class="dv">1</span>]</span>
<span id="cb4-394"><a href="#cb4-394" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-395"><a href="#cb4-395" aria-hidden="true" tabindex="-1"></a>    <span class="co">## projection with the eigenvalues of sigma</span></span>
<span id="cb4-396"><a href="#cb4-396" aria-hidden="true" tabindex="-1"></a>    Eig<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb4-397"><a href="#cb4-397" aria-hidden="true" tabindex="-1"></a>    logeig<span class="op">=</span>np.sort(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>])</span>
<span id="cb4-398"><a href="#cb4-398" aria-hidden="true" tabindex="-1"></a>    delta<span class="op">=</span>np.zeros(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-399"><a href="#cb4-399" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(logeig)<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb4-400"><a href="#cb4-400" aria-hidden="true" tabindex="-1"></a>        delta[j]<span class="op">=</span><span class="bu">abs</span>(logeig[j]<span class="op">-</span>logeig[j<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb4-401"><a href="#cb4-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-402"><a href="#cb4-402" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>np.argmax(delta)<span class="op">+</span><span class="dv">1</span>         <span class="co"># biggest gap between the l(lambda_i)</span></span>
<span id="cb4-403"><a href="#cb4-403" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-404"><a href="#cb4-404" aria-hidden="true" tabindex="-1"></a>    indi<span class="op">=</span>[]</span>
<span id="cb4-405"><a href="#cb4-405" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb4-406"><a href="#cb4-406" aria-hidden="true" tabindex="-1"></a>        indi.append(np.where(np.log(Eig[<span class="dv">0</span>])<span class="op">-</span>Eig[<span class="dv">0</span>]<span class="op">==</span>logeig[l])[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb4-407"><a href="#cb4-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-408"><a href="#cb4-408" aria-hidden="true" tabindex="-1"></a>    P1<span class="op">=</span>np.array(Eig[<span class="dv">1</span>][:,indi[<span class="dv">0</span>]],ndmin<span class="op">=</span><span class="dv">2</span>).T                  </span>
<span id="cb4-409"><a href="#cb4-409" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,k):</span>
<span id="cb4-410"><a href="#cb4-410" aria-hidden="true" tabindex="-1"></a>        <span class="co"># matrix of inflential directions of projection</span></span>
<span id="cb4-411"><a href="#cb4-411" aria-hidden="true" tabindex="-1"></a>        P1<span class="op">=</span>np.concatenate((P1,np.array(Eig[<span class="dv">1</span>][:,indi[l]],ndmin<span class="op">=</span><span class="dv">2</span>).T),axis<span class="op">=</span><span class="dv">1</span>)   </span>
<span id="cb4-412"><a href="#cb4-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-413"><a href="#cb4-413" aria-hidden="true" tabindex="-1"></a>    diagsi<span class="op">=</span>np.diag(Eig[<span class="dv">0</span>][indi])</span>
<span id="cb4-414"><a href="#cb4-414" aria-hidden="true" tabindex="-1"></a>    sig_opt_d<span class="op">=</span>P1.dot((diagsi<span class="op">-</span>np.eye(k))).dot(P1.T)<span class="op">+</span>np.eye(d)  </span>
<span id="cb4-415"><a href="#cb4-415" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-416"><a href="#cb4-416" aria-hidden="true" tabindex="-1"></a>    DKL[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sigma))<span class="op">+</span>np.<span class="bu">sum</span>(np.diag(<span class="op">\</span></span>
<span id="cb4-417"><a href="#cb4-417" aria-hidden="true" tabindex="-1"></a>                    Sigstar.dot(np.linalg.inv(sigma))))</span>
<span id="cb4-418"><a href="#cb4-418" aria-hidden="true" tabindex="-1"></a>    DKLp[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(sig_opt_d))<span class="op">+</span>np.<span class="bu">sum</span>(<span class="op">\</span></span>
<span id="cb4-419"><a href="#cb4-419" aria-hidden="true" tabindex="-1"></a>                    np.diag(Sigstar.dot(np.linalg.inv(sig_opt_d))))</span>
<span id="cb4-420"><a href="#cb4-420" aria-hidden="true" tabindex="-1"></a>    DKLstar[<span class="bu">int</span>((d<span class="op">-</span><span class="dv">5</span>)<span class="op">/</span><span class="dv">5</span>)]<span class="op">=</span>np.log(np.linalg.det(Sigstar))<span class="op">+</span>d</span>
<span id="cb4-421"><a href="#cb4-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-422"><a href="#cb4-422" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of partial KL divergence</span></span>
<span id="cb4-423"><a href="#cb4-423" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKL,<span class="st">'bo'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*)$"</span>)</span>
<span id="cb4-424"><a href="#cb4-424" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLstar,<span class="st">'rs'</span>,label<span class="op">=</span><span class="vs">r"$D'(\mathbf{\Sigma}^*)$"</span>)</span>
<span id="cb4-425"><a href="#cb4-425" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">5</span>,<span class="dv">105</span>,<span class="dv">5</span>),DKLp,<span class="st">'k.'</span>,label<span class="op">=</span><span class="vs">r"$D'(\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*_k)$"</span>)</span>
<span id="cb4-426"><a href="#cb4-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-427"><a href="#cb4-427" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb4-428"><a href="#cb4-428" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Dimension'</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-429"><a href="#cb4-429" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"Partial KL divergence $D'$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-430"><a href="#cb4-430" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-431"><a href="#cb4-431" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb4-432"><a href="#cb4-432" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb4-433"><a href="#cb4-433" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-434"><a href="#cb4-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-435"><a href="#cb4-435" aria-hidden="true" tabindex="-1"></a><span class="co">#### plot of the eigenvalues</span></span>
<span id="cb4-436"><a href="#cb4-436" aria-hidden="true" tabindex="-1"></a>Eig1<span class="op">=</span>np.linalg.eigh(sigma)</span>
<span id="cb4-437"><a href="#cb4-437" aria-hidden="true" tabindex="-1"></a>logeig1<span class="op">=</span>np.log(Eig1[<span class="dv">0</span>])<span class="op">-</span>Eig1[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb4-438"><a href="#cb4-438" aria-hidden="true" tabindex="-1"></a>Table_eigv<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb4-439"><a href="#cb4-439" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">0</span>]<span class="op">=</span>Eig1[<span class="dv">0</span>]</span>
<span id="cb4-440"><a href="#cb4-440" aria-hidden="true" tabindex="-1"></a>Table_eigv[:,<span class="dv">1</span>]<span class="op">=-</span>logeig1</span>
<span id="cb4-441"><a href="#cb4-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-442"><a href="#cb4-442" aria-hidden="true" tabindex="-1"></a>Eigst<span class="op">=</span>np.linalg.eigh(Sigstar)</span>
<span id="cb4-443"><a href="#cb4-443" aria-hidden="true" tabindex="-1"></a>logeigst<span class="op">=</span>np.log(Eigst[<span class="dv">0</span>])<span class="op">-</span>Eigst[<span class="dv">0</span>]<span class="op">+</span><span class="dv">1</span></span>
<span id="cb4-444"><a href="#cb4-444" aria-hidden="true" tabindex="-1"></a>Table_eigv_st<span class="op">=</span>np.zeros((n,<span class="dv">2</span>))</span>
<span id="cb4-445"><a href="#cb4-445" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">0</span>]<span class="op">=</span>Eigst[<span class="dv">0</span>]</span>
<span id="cb4-446"><a href="#cb4-446" aria-hidden="true" tabindex="-1"></a>Table_eigv_st[:,<span class="dv">1</span>]<span class="op">=-</span>logeigst</span>
<span id="cb4-447"><a href="#cb4-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-448"><a href="#cb4-448" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb4-449"><a href="#cb4-449" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"Eigenvalues $\lambda_i$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-450"><a href="#cb4-450" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\ell(\lambda_i)$"</span>,fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-451"><a href="#cb4-451" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> tickLabel <span class="kw">in</span> plt.gca().get_xticklabels() <span class="op">+</span> plt.gca().get_yticklabels():</span>
<span id="cb4-452"><a href="#cb4-452" aria-hidden="true" tabindex="-1"></a>    tickLabel.set_fontsize(<span class="dv">16</span>)</span>
<span id="cb4-453"><a href="#cb4-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-454"><a href="#cb4-454" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv[:,<span class="dv">0</span>],Table_eigv[:,<span class="dv">1</span>],<span class="st">'bx'</span>,<span class="op">\</span></span>
<span id="cb4-455"><a href="#cb4-455" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="vs">r"Eigenvalues of $\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>)</span>
<span id="cb4-456"><a href="#cb4-456" aria-hidden="true" tabindex="-1"></a>plt.plot(Table_eigv_st[:,<span class="dv">0</span>],Table_eigv_st[:,<span class="dv">1</span>],<span class="st">'rs'</span>,<span class="op">\</span></span>
<span id="cb4-457"><a href="#cb4-457" aria-hidden="true" tabindex="-1"></a>         label<span class="op">=</span><span class="vs">r"Eigenvalues of $\mathbf{\Sigma}^*$"</span>)</span>
<span id="cb4-458"><a href="#cb4-458" aria-hidden="true" tabindex="-1"></a>plt.legend(fontsize<span class="op">=</span><span class="dv">16</span>)</span>
<span id="cb4-459"><a href="#cb4-459" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb4-460"><a href="#cb4-460" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-461"><a href="#cb4-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-462"><a href="#cb4-462" aria-hidden="true" tabindex="-1"></a><span class="fu">###  Numerical results</span></span>
<span id="cb4-463"><a href="#cb4-463" aria-hidden="true" tabindex="-1"></a>We report in @tbl-sum the numerical results for the six different matrices and the vMFN model for the dimension $n=100$. The column $\mathbf{\Sigma}^*$ gives the optimal results, while the column $\widehat{\mathbf{\Sigma}}^*$ corresponds to the results that we are trying to improve. Comparing these two columns, we notice as expected that the estimation of $\mathcal{E}$ with $\widehat{\mathbf{\Sigma}}^*$ is significantly degraded. Compared to the first column $\mathbf{\Sigma}^*$, the third and fourth columns with ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}} =  {\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ correspond to the best projection direction $\textbf{1}$ (as for $\mathbf{\Sigma}^*$) but estimating the variance in this direction (instead of the true variance) with $\textbf{1}^\top \widehat{\mathbf{\Sigma}}^* \textbf{1}$. This choice performs very well, with numerical results similar to the optimal ones. This can be understood since in this case, both ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ and $\mathbf{\Sigma}^*$ are of the form $\alpha \textbf{1} \textbf{1}^\top + I_n$ and so estimating ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ requires only a one-dimensional estimation (namely, the estimation of $\alpha$). Next, the last two columns ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ highlight the impact of having to estimate the projection directions in addition to the variance since these two matrices are of the form $\widehat \alpha \widehat{\textbf{1}} {\widehat{\textbf{1}}}^\top + I_n$ with both $\widehat{\alpha}$ (the variance term) and $\widehat{\textbf{1}}$ (the direction) being estimated. We observe that these matrices yield results which are close to optimal and greatly improve the estimation obtained using $\widehat{\mathbf{\Sigma}}^*$. </span>
<span id="cb4-464"><a href="#cb4-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-465"><a href="#cb4-465" aria-hidden="true" tabindex="-1"></a>Moreover, we observe that ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$ gives better results than ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{opt}}$. We suggest that this is because $\widehat{\mathbf{m}}^* / \lVert \widehat{\mathbf{m}}^* \rVert$ is a better estimator of $\textbf{1}$ than the eigenvector of $\widehat{\mathbf{\Sigma}}^*$. Indeed, evaluating $\widehat{\mathbf{m}}^*$ requires the estimation of $n$ parameters, whereas $\widehat{\mathbf{\Sigma}}^*$ needs around $n^2/2$ parameters to estimate, so the eigenvector is finally more noisy than the mean vector.  In the last column, we present the vMFN estimation that is slightly more efficicent than the estimation obtained with ${\widehat{\mathbf{\Sigma}}^{\text{+d}}_\text{mean}}$.</span>
<span id="cb4-466"><a href="#cb4-466" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-467"><a href="#cb4-467" aria-hidden="true" tabindex="-1"></a>Thus, the proposed idea improves significantly the probability estimation in high dimensions. But we see that the method taken in <span class="co">[</span><span class="ot">@MasriEtAl_ImprovementCrossentropyMethod_2020</span><span class="co">]</span> with the projection $\mathbf{m}^*$ is at least as much efficient in this example where we need only a one-dimensional projection. The next case shows that the projection on more than one direction can outperform the one-dimensional projection on $\mathbf{m}^*$. </span>
<span id="cb4-468"><a href="#cb4-468" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-471"><a href="#cb4-471" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb4-472"><a href="#cb4-472" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: tbl-sum</span></span>
<span id="cb4-473"><a href="#cb4-473" aria-hidden="true" tabindex="-1"></a><span class="co">#| tbl-cap: 'Numerical comparison of the estimation of $\mathcal{E} \approx 1.35\cdot 10^{-3}$ considering the Gaussian model with the six covariance matrices defined in @sec-def_cov and the vFMN model, when $\phi = \mathbb{I}_{\{\varphi\geq 0\}}$ with $\varphi$ the linear function given by @eq-sum. As explained in the text, ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ are actually equal in this case. The computational cost is $N=2000$.'</span></span>
<span id="cb4-474"><a href="#cb4-474" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb4-475"><a href="#cb4-475" aria-hidden="true" tabindex="-1"></a><span class="co"># Table 2. Numerical comparison on test case 1</span></span>
<span id="cb4-476"><a href="#cb4-476" aria-hidden="true" tabindex="-1"></a><span class="co">###########################################################################</span></span>
<span id="cb4-477"><a href="#cb4-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-478"><a href="#cb4-478" aria-hidden="true" tabindex="-1"></a>n<span class="op">=</span><span class="dv">100</span>         <span class="co"># dimension</span></span>
<span id="cb4-479"><a href="#cb4-479" aria-hidden="true" tabindex="-1"></a>phi<span class="op">=</span>Somme</span>
<span id="cb4-480"><a href="#cb4-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-481"><a href="#cb4-481" aria-hidden="true" tabindex="-1"></a>Tabresult <span class="op">=</span> np.zeros((<span class="dv">3</span>,<span class="dv">7</span>)) <span class="co"># table of results</span></span>
<span id="cb4-482"><a href="#cb4-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-483"><a href="#cb4-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-484"><a href="#cb4-484" aria-hidden="true" tabindex="-1"></a>table<span class="op">=</span>[[<span class="st">"D'"</span>,Tabresult[<span class="dv">0</span>,<span class="dv">0</span>],Tabresult[<span class="dv">0</span>,<span class="dv">1</span>],Tabresult[<span class="dv">0</span>,<span class="dv">2</span>],Tabresult[<span class="dv">0</span>,<span class="dv">3</span>],</span>
<span id="cb4-485"><a href="#cb4-485" aria-hidden="true" tabindex="-1"></a>        Tabresult[<span class="dv">0</span>,<span class="dv">4</span>],Tabresult[<span class="dv">0</span>,<span class="dv">5</span>],<span class="st">"/"</span>],</span>
<span id="cb4-486"><a href="#cb4-486" aria-hidden="true" tabindex="-1"></a>      [<span class="vs">r"Relative error (\%)"</span>,Tabresult[<span class="dv">1</span>,<span class="dv">0</span>],Tabresult[<span class="dv">1</span>,<span class="dv">1</span>],</span>
<span id="cb4-487"><a href="#cb4-487" aria-hidden="true" tabindex="-1"></a>       Tabresult[<span class="dv">1</span>,<span class="dv">2</span>],Tabresult[<span class="dv">1</span>,<span class="dv">3</span>],Tabresult[<span class="dv">1</span>,<span class="dv">4</span>],Tabresult[<span class="dv">1</span>,<span class="dv">5</span>],Tabresult[<span class="dv">1</span>,<span class="dv">6</span>]],</span>
<span id="cb4-488"><a href="#cb4-488" aria-hidden="true" tabindex="-1"></a>    [<span class="vs">r"Coefficient of variation (\%)"</span>,Tabresult[<span class="dv">2</span>,<span class="dv">0</span>],Tabresult[<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb4-489"><a href="#cb4-489" aria-hidden="true" tabindex="-1"></a>     Tabresult[<span class="dv">2</span>,<span class="dv">2</span>],Tabresult[<span class="dv">2</span>,<span class="dv">3</span>],Tabresult[<span class="dv">2</span>,<span class="dv">4</span>],Tabresult[<span class="dv">2</span>,<span class="dv">5</span>],Tabresult[<span class="dv">2</span>,<span class="dv">6</span>]]]</span>
<span id="cb4-490"><a href="#cb4-490" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-491"><a href="#cb4-491" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb4-492"><a href="#cb4-492" aria-hidden="true" tabindex="-1"></a>  table, </span>
<span id="cb4-493"><a href="#cb4-493" aria-hidden="true" tabindex="-1"></a>  headers<span class="op">=</span>[<span class="st">""</span>, <span class="vs">r"$\mathbf{\Sigma}^*$"</span>, <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^*$"</span>,</span>
<span id="cb4-494"><a href="#cb4-494" aria-hidden="true" tabindex="-1"></a>       <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_</span><span class="sc">{opt}</span><span class="vs">$"</span>,</span>
<span id="cb4-495"><a href="#cb4-495" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">_\text</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="vs">r"${\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{opt}</span><span class="vs">}$"</span>,</span>
<span id="cb4-496"><a href="#cb4-496" aria-hidden="true" tabindex="-1"></a>           <span class="vs">r"$\widehat{\mathbf{\Sigma</span><span class="sc">}}</span><span class="vs">^{+d}_</span><span class="sc">{mean}</span><span class="vs">$"</span>, <span class="st">"vMFN"</span>],</span>
<span id="cb4-497"><a href="#cb4-497" aria-hidden="true" tabindex="-1"></a>  tablefmt<span class="op">=</span><span class="st">"latex"</span>))</span>
<span id="cb4-498"><a href="#cb4-498" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb4-499"><a href="#cb4-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-500"><a href="#cb4-500" aria-hidden="true" tabindex="-1"></a><span class="fu">##  Test case 2: projection in 2 directions   {#sec-sub:parabol}    </span></span>
<span id="cb4-501"><a href="#cb4-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-502"><a href="#cb4-502" aria-hidden="true" tabindex="-1"></a>The second test case is again a probability estimation, i.e., it is of the form $\phi = \mathbb{I}_{<span class="sc">\{</span>\varphi \geq 0<span class="sc">\}</span>}$ with now the function $\varphi$ having some quadratic terms:</span>
<span id="cb4-503"><a href="#cb4-503" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-504"><a href="#cb4-504" aria-hidden="true" tabindex="-1"></a>    \varphi: \mathbf{x}=(x_1,\ldots,x_n) \in \mathbb{R}^n \mapsto x_1 - 25 x_2^2 - 30 x_3^2 - 1.</span>
<span id="cb4-505"><a href="#cb4-505" aria-hidden="true" tabindex="-1"></a>$$ {#eq-parabol}</span>
<span id="cb4-506"><a href="#cb4-506" aria-hidden="true" tabindex="-1"></a>        The quantity of interest $\mathcal{E}$ is defined as $\mathcal{E}=\int_{\mathbb{R}^n} \phi(\mathbf{x}) f(\mathbf{x}) \textrm{d}\mathbf{x} = \mathbb{P}_f(\varphi(\mathbf{X})\geq 0)$ for all $n$ where the density $f$ is the standard $n$-dimensional Gaussian distribution. This function is motivated in part because $\mathbf{m}^*$ and $\mathbf{d}^*_1$ are different and also because Algorithm 2 chooses two projection directions. Thus, this is an example where ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{mean}}$ and ${\widehat{\mathbf{\Sigma}}^{\text{}}_\text{opt}}$ are significantly different.</span>
<span id="cb4-507"><a href="#cb4-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-508"><a href="#cb4-508" aria-hidden="true" tabindex="-1"></a><span class="fu">###  Evolution of the partial KL divergence and spectrum</span></span>
<span id="cb4-509"><a href="#cb4-509" aria-hidden="true" tabindex="-1"></a>We check on @fig-inefficiency-parab-1 that the partial KL divergence obeys the same behavior as for the previous example, namely the one associated with $\widehat{\mathbf{\Sigma}}^*$ increases much faster than the ones associated with $\mathbf{\Sigma}^*$ and $\widehat{\mathbf{\Sigma}}^*_k$, which again suggests that projecting can improve the situation. Since the function $\varphi$ only depends on the first three variables and is even in $x_2$ and $x_3$, one gets that $\mathbf{m}^* = \alpha </span>
<span id="cb4-510"><a href="#cb4-510" aria-hidden="true" tabindex="-1"></a>    \textbf{e}_1$ with $\alpha = \mathbb{E}(X_1 \mid X_1 \geq 25 X^2_2 + 30 X^2_3 + 1) \approx 1.9$ (here and in the sequel, $\textbf{e}_i$ denotes the $i$th canonical vector of $\mathbb{R}^n$, i.e., all its coordinates are $0$ except the $i$-th one which is equal to one), and that $\mathbf{\Sigma}^*$ is diagonal with</span>
<span id="cb4-511"><a href="#cb4-511" aria-hidden="true" tabindex="-1"></a>$$ \mathbf{\Sigma}^* =</span>
<span id="cb4-512"><a href="#cb4-512" aria-hidden="true" tabindex="-1"></a>    \begin{pmatrix}</span>
<span id="cb4-513"><a href="#cb4-513" aria-hidden="true" tabindex="-1"></a>    \lambda_1 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb4-514"><a href="#cb4-514" aria-hidden="true" tabindex="-1"></a>    0 &amp; \lambda_2 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb4-515"><a href="#cb4-515" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; \lambda_3 &amp; 0 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb4-516"><a href="#cb4-516" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 <span class="sc">\\</span></span>
<span id="cb4-517"><a href="#cb4-517" aria-hidden="true" tabindex="-1"></a>    \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots <span class="sc">\\</span></span>
<span id="cb4-518"><a href="#cb4-518" aria-hidden="true" tabindex="-1"></a>    0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 <span class="sc">\\</span></span>
<span id="cb4-519"><a href="#cb4-519" aria-hidden="true" tabindex="-1"></a>    \end{pmatrix}. </span>
<span id="cb4-520"><a href="#cb4-520" aria-hidden="true" tabindex="-1"></a> $$</span>
<span id="cb4-521"><a href="#cb4-521" aria-hidden="true" tabindex="-1"></a>Note that the off-diagonal elements of the submatrix $(\mathbf{\Sigma}^*_{ij})_{1 \leq i, j \leq 3}$ are indeed $0$ since they result from integrating an odd function of an odd random variable with an even conditioning. For instance, if $F(x) = \mathbb{P}(30 X^2_3 + 1 \leq x)$, then by conditioning on $(X_1, X_3)$ we obtain</span>
<span id="cb4-522"><a href="#cb4-522" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-523"><a href="#cb4-523" aria-hidden="true" tabindex="-1"></a>    \mathbf{\Sigma}^*_{12} = \mathbb{E} \left( (X_1 - \alpha) X_2 \mid X_1 - 25 X_2^2 \geq 30 X^2_3 + 1 \right)<span class="sc">\\</span></span>
<span id="cb4-524"><a href="#cb4-524" aria-hidden="true" tabindex="-1"></a>     = \frac{1}{\mathcal{E}} \mathbb{E} \left<span class="co">[</span><span class="ot"> (X_1 - \alpha) \mathbb{E} \left( X_2 F(X_1 - 25 X^2_2) \mid X_1 \right) \right</span><span class="co">]</span></span>
<span id="cb4-525"><a href="#cb4-525" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb4-526"><a href="#cb4-526" aria-hidden="true" tabindex="-1"></a>    which is $0$ as $x_2 F(x_1 - x^2_2)$ is an odd function of $x_2$ for fixed $x_1$, and $X_2$ has an even density. </span>
<span id="cb4-527"><a href="#cb4-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-528"><a href="#cb4-528" aria-hidden="true" tabindex="-1"></a>We can numerically compute $\lambda_1 \approx 0.28$, $\lambda_2 \approx 0.009$ and $\lambda_3 \approx 0.008$. These values correspond to the red squares in @fig-inefficiency-parab-2 which shows that the smallest eigenvalues are properly estimated. Moreover, Algorithm 2 selects the two largest eigenvalues, which have the highest $\ell$-values. These two eigenvalues thus correspond to the eigenvectors $\mathbf{e}_2$ and $\mathbf{e}_3$, and so we see that on this example, the optimal directions predicted by @thm-thm1 are significantly different (actually, orthogonal) from $\mathbf{m}^*$ which is proportional to $\textbf{e}_1$.</span>
<span id="cb4-529"><a href="#cb4-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-530"><a href="#cb4-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-531"><a href="#cb4-531" aria-hidden="true" tabindex="-1"></a><span class="fu"># References {.unnumbered}</span></span>
<span id="cb4-532"><a href="#cb4-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-533"><a href="#cb4-533" aria-hidden="true" tabindex="-1"></a>::: {#refs}</span>
<span id="cb4-534"><a href="#cb4-534" aria-hidden="true" tabindex="-1"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<script>
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>
<script>
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    let pseudocodeOptions = {
      indentSize: el.dataset.indentSize || "1.2em",
      commentDelimiter: el.dataset.commentDelimiter || "//",
      lineNumber: el.dataset.lineNumber === "true" ? true : false,
      lineNumberPunc: el.dataset.lineNumberPunc || ":",
      noEnd: el.dataset.noEnd === "true" ? true : false,
      titlePrefix: el.dataset.algTitle || "Algorithm"
    };
    pseudocode.renderElement(el.querySelector(".pseudocode"), pseudocodeOptions);
  });
})(document);
(function(d) {
  d.querySelectorAll(".pseudocode-container").forEach(function(el) {
    titleSpan = el.querySelector(".ps-root > .ps-algorithm > .ps-line > .ps-keyword")
    titlePrefix = el.dataset.algTitle;
    titleIndex = el.dataset.chapterLevel ? el.dataset.chapterLevel + "." + el.dataset.pseudocodeIndex : el.dataset.pseudocodeIndex;
    titleSpan.innerHTML = titlePrefix + " " + titleIndex + " ";
  });
})(document);
</script>




</body></html>